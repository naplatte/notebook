## 一、基本概念

- > 强化学习讨论的问题：一个智能体（**agent**）如何在一个复杂的环境（**environment**）中去极大化它所获得的奖励。通过感知**环境的状态**（**state**）对动作（**action**）的反应（**reward**），来指导更好的动作，从而获得最大的收益（**return**）。以上过程称为在交互中学习，这样的学习方法称为强化学习。
- 一个例子：机器人走网格

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/453549f5955797d8b71c581e44ff9d81.png" alt="image-20250415110836537" style="zoom:80%;" />	

- **state**：agent相对于environment的一个status，在上面的例子中，state就是指位置，使用s1-s9表示这些位置状态；**环境决定状态**

- **state space**（状态空间）：即所有状态的集合

- **Action**：agent在某一时刻根据当前state做出决策/选择的行为，在上面例子中，在一个位置状态下，可能有5个action（向上下左右走&原地不动）
- 离散的动作空间：动作的选择是可数的，比如这里的上下左右，常用算法：Q-learning

- 连续的动作空间：动作类型是一个实数区间（无限可能），比如手臂的力度
- **Action space of state**：当前状态下所有的action集合 - 每个状态都有一个action space

- **state transition**：agent在某一state下执行某种action，转移到另一个state
  - 如果action触碰到边界，则相当于agent原地不动
  - 如果action进入到forbidden area，有两种情况：①forbidden area可进入，则直接进入；②forbidden area不可进入，则相当于原地不动

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/d958291fea7edcd0be81c439ab31a136.png" alt="image-20250415110900821" style="zoom:67%;" />	

- **state transition probability**：借助条件概率去描述状态转移
- 比如当前在s1，执行动作a2后，若P(s2|s1,a2) = 1，则说明一定会到达s2
  - 若P(s2|s1,a2) = 0.5，P(s5|s1,a2) = 0.5，则说明有50%的概率到达s2，有50%的概率到达s5
- **policy(策略）**：根据当前state，决定采取什么action
- **Reward(奖励)**：环境给agent一个反馈，用来指导agent是不是做得好，比如机器人走到出口 + 1分，机器人撞墙 - 1分
- **Episode(回合)**：agent从起点出发，直到任务结束，称为一轮，一个episode是一条完整的trajectory，**从起始状态开始，到终止状态结束**
- **Trajectory(轨迹)**：agent一轮行动中，形如**(状态，动作，奖励)**这样的**序列**
- **Return(回报)**：从当前开始，到终点累计的总奖励（Reward之和），强化学习的目标就是最大化期望的Return，Return可以用来评估Policy的好坏

## 二、状态值与贝尔曼方程 

> 本章需要搞明白一个核心概念和一个核心工具，核心概念：状态值，核心工具：贝尔曼方程；需要使用核心工具 - 贝尔曼方程 来分析核心概念 - 状态值

### 1.关于回报

- 首先复习一下Return，Return就是从当前开始，到终点累计的Reward之和，这个Return针对的是一条轨迹，所以说Return是重要的，Return可以用来评价策略的好坏

- 如何计算Return

  - 法一：根据定义：Return就是沿轨迹收集的Reward之和，依次计算每次行动的Reward然后相加即可

  - 法二：自举法，类似于一个循环递归，简单说就是**不同状态出发的Reward是彼此依赖的**，对于下面的例子v1 - v4都是未知的，四个未知数四个方程，联立即可解出所有Return

    <img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/50478589b148b7932de4c2f48e51a466.png" alt="image-20250606112204576" style="zoom:67%;" />23	

### 2.状态值

- 对于一条轨迹，我们可以根据定义得到折扣回报

​		<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/39ce7b9c96a497093bc64e2ca3319d27.png" alt="image-20250606135651511" style="zoom:67%;" />	

- 上文的Return针对的是一条轨迹，而在一个state下，可能会有多条轨迹可以到达终点，对于每一条轨迹，我们都可以计算出一个Return，这里就引出了状态值的概念
- 状态值：对于一个state，所有轨迹的Return的期望（平均），即为状态值，状态值依赖于策略，依赖于状态，但不依赖于时间
- 状态值：在策略π下，从当前状态出发，未来所有可能轨迹回报的期望
- 不同的策略可能得到相同的state Value

### 3.贝尔曼方程

- 贝尔曼方程：描述了所有状态值之间的关系
- 推导过程：首先是折扣回报的推导，其等于当前即时奖励 + 未来奖励

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/890c5ec4c0c2712c8fc404b7a02fd4a2.png" alt="image-20250608132721268" style="zoom:80%;" />	

- 上文提到状态值就是所有轨迹Return的期望，因此借助期望引入状态值的推导

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/ae9faf4bf393cf5c140f340084cac873.png" alt="image-20250608133046416" style="zoom:80%;" />	

- 第一项为**即时奖励的期望值**，在状态s有很多action可以take，我们假设take a的概率是π，第一行可以以全概率公式的角度去理解，第二行就是期望的定义，$p(r|s,a)r$即在状态s下take a，获得奖励为r的概率，再乘r，累加就是一个算期望的过程，感觉也可以以全概率公式的角度去理解

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/51d6626e4a10ce1390b57b0fbced2c4f.png" alt="image-20250608133709273" style="zoom:80%;" />	

- 第二项为**未来奖励的期望**，由马尔科夫性，未来奖励进依赖于当前状态，与先前状态无关，故$S_{t}=s$可去，因为当前求的是t+1时刻的奖励，最外层的求和也去按全概率的角度理解，最后两行也不难理解

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/134ef5b7174acc5051ba953149ae804f.png" alt="image-20250608135643115" style="zoom:80%;" />	

- 最终形式：整合上面二者

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/22e6234afcdbf2fd691a741cb6463bfd.png" alt="image-20250608140207061" style="zoom:80%;" />	

- 矩阵向量形式：状态值 = 即时奖励 + 折扣因子 * 状态转移概率矩阵 * 状态值，直观上理解也是状态值 = 当前即时奖励 + 未来奖励，其中P是一个矩阵，状态值V是一个向量，PV相乘后的向量即为每个状态的未来价值期望

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/0425ee14abade8f93ceb9d9ea84710bc.png" alt="image-20250608142030360" style="zoom:80%;" />	

### 4.动作值

- 定义：动作值定义为**一个状态采取一个动作之后**获得的回报的期望值
- 对比状态值
  - 状态值是对当前状态下，多个策略（轨迹）得到的Return求期望 - **从当前状态s出发的长期收益**
  - 动作值是在当前状态下，选择不同action得到Return（折扣奖励）的期望 - **从当前状态s选择动作a后的长期收益**
- 计算：可由状态值公式表示

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/442df695974d02c9b7f23d5d4c592c9a.png" alt="image-20250608144001622" style="zoom:80%;" />	

- 即使一个动作不会被策略选择，其依然具有动作值
- 强化学习的目的是寻找最优策略，因此必须探索每个状态下的所有动态，找到最优动作

## 三、最优状态值与贝尔曼最优方程

> 核心概念：最优状态值，基于此可以**定义**最优策略；
>
> 核心工具：贝尔曼最优方程，基于此，可以**求解**最优状态值和最优策略

### 1.启发例子：策略是否可改进

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/5c84c4a3ec01f15257caf6d7b666d82c.png" alt="image-20250608151040101" style="zoom: 67%;" />	

- 显然在直观上，这个策略是有问题的，因为在s1时选择的action会进入forbidden area，利用上节介绍的动作值，我们也可以计算出，在s1状态时，向下移动对应的动作值是最大的
- 如果我们更新策略从而使选择具有最大动作值的动作，就有望得到一个更好的策略

### 2.最优策略与贝尔曼最优方程

- 最优策略：在每一个状态都有比其他策略更高的状态值

- 一个状态的最优价值 = 在遵循最优策略时所获得的期望回报
- 贝尔曼最优方程：一个方程有两个未知数，分别为最优状态值v(s)和最优策略π
  - 贝尔曼最优方程就是用来描述**最优策略下的价值函数应该满足的关系**
  - **最优策略π选择具有最大动作值的动作**，方程转换为一个未知数，进而可求出最优状态值
  

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/b84d8d2cee49a3a1913f689f0b6b3d31.png" alt="image-20250608161433833" style="zoom: 67%;" />	

- 矩阵向量形式：类似上一节的贝尔曼方程，贝尔曼最优方程也有对应的矩阵向量形式

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/912272295670dea6d3cfd47bb9d48ee0.png" alt="image-20250610093900279" style="zoom:67%;" />	

- 上面的$\max_{\pi}(r_\pi+\gamma P_\pi v)$可以表示为$f(v)$，如何理解
  - 本质：**对每个状态s，考虑所有可能的策略$\pi$，在当前状态值$v$的基础上，选择一个策略，使在这个策略下即时奖励r+未来回报期望$\gamma P_\pi v$最大**

> 按我的思路帮我捋一下：
> 首先贝尔曼方程，它是研究状态值的工具对吧，每个状态都有一个贝尔曼方程对吧，在某个状态下，其状态值 = 即时奖励 + 未来回报期望对吧✔️
>
> 然后是贝尔曼最优方程，它应该也是求一个状态值对吗，既然是状态值，也就是说每一个状态都有一个贝尔曼最优方程对吧，当前状态下，按照贝尔曼最优方程求出的状态值，应该是最优状态值是吗，这个最优状态值，是在当前状态下选择最优策略得出的是吗✔️但是这个方程有个问题，就是它有两个未知量（v和π）对吗，这个策略π，就是在当前状态选择最优动作值的action对吗✔️那这个策略π表示的是一个动作还是多个动作？如果是多个动作的话，当前状态只能求在当前状态的动作值吧，怎么去选以后的动作，如果是一个动作就没问题了，当我没说好了。**（⭐一个动作，在当前状态下选择动作值最大的动作，每个状态都如此，最终就是最优策略）**
>
> 再回到我上面写成f(v)这个问题，这里是针对的矩阵向量形式，我能不能先考虑常规形式，也就是在某一状态下，我输入一个状态值，然后寻找在所有策略下，这个状态值什么时候最大对吗？但是这个输入的状态值是什么？是一个随机值吗？我不太懂这个f(v)的过渡，按照贝尔曼方程，它的目的就是去求出在当前状态下的最优状态值，而转换为矩阵向量形式时，为什么就变成了输入一个状态值v，求一个最优状态值，我不明白**==（强化学习的迭代过程就是“猜”+“逼近”，在初始时我们不知道最优V，那就猜一个，然后反复去用贝尔曼最优方程去更新它，直到收敛，这个更新过程就是函数映射，即输入一个V，通过计算在哪个策略下V最大，得到的最大V再作为函数输入，直到V收敛到一个不动点V*）==**

### 3.如何求解f(v)

#### ①数学前提知识

- 不动点：对于$x$，如果有$f(x) = x$ ，则x被称为不动点，即自变量经过函数映射后依然等于它本身
- 压缩映射：$$||f(x_1) - f(x_2)|| <= \gamma ||x_1 - x_2||$$，直观理解就是，两点函数值的差<=其本身的差
- 压缩映射理论：对于方程$x = f(x)$，其中x和f(x)是实数向量，如果$f$是一个压缩映射，则下面性质成立
  - 存在性：必存在一个不动点$x^*$使$f(x^*)=x^*$
  - 唯一性：上面的$x^*$是唯一的
  - 如何求这个不动点 - 迭代算法：$x_{k+1}=f(x_k)$，给定一个任意$x_0$，一直迭代，$k\rightarrow \infty$时，$x_k \rightarrow x^*$，其实就是这样一个数列的极限 

#### ②f(v)的压缩性质 - 进而求解$v^*$与$\pi ^*$

- 给出不加证明的结论，f(v)可证得具有压缩映射的性质，即$$||f(v_1) - f(v_2)|| <= \gamma ||v_1 - v_2||$$
- 求解$v^*$：因此贝尔曼最优公式满足压缩映射性质，我们可以有：此公式一定有解，且解唯一，可以使用迭代算法去求解，因此我们上面说，给一个初始$v$值去迭代，迭代到最后$v$会收敛到$v^*$，即最优状态值
- 求解$\pi *$：得到$v^*$后，带入到贝尔曼方程可以求出$\pi^*$，即最优策略，**这个最优策略，实际上就是，对于当前状态s，选择动作值最大的动作的概率为1，选择其他动作的概率为0，每个状态都这样做，最终就形成了最优策略**

### 4.最优策略的有趣性质

- 影响最优策略的因素：看贝尔曼方程即可得知，v和π是未知量，$r , \gamma,p(r|s,a)$是已知量，故后面三者是影响最优策略的因素，而关于$p(r|s,a)$，其为系统模型，故一般来说$r,\gamma$是影响最优策略的因素
  - 改变$r,\gamma$都可能会使策略发生变化
- 关于奖励的绝对值与相对值：真正影响策略的是奖励的相对值，也就是说对所有奖励进行同比例放缩，最优策略是保持不变的
- 在第一章我们也说过，“奖励”实际上是一个人机交互的工具，因此我们可以通过调整奖励来得到想要的策略
- 避免无意义的绕路：最优策略会避免无意义的绕路

##  四、值迭代与策略迭代

- 引言：从这章开始学习强化学习算法，本章介绍三种方法：值迭代、策略迭代以及截断策略迭代，其中值迭代和策略迭代都是截断策略迭代的特殊情况，因此截断策略迭代更具一般化
- 本章介绍的三种方法都是基于模型的，即我们需要实现知道系统模型，**拥有对环境的完整知识，知道环境的状态转移概率和奖励函数**

### 1.值迭代算法

- 值迭代的方法基于贝尔曼最优方程

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/eef572175d6c2109d731cdd72c2b36f2.png" alt="image-20250611162704815" style="zoom:67%;" />	

- 每次迭代的两个步骤
  1. 策略更新：$\pi_{k+1}={argmax}_\pi(r_\pi + \gamma P_\pi v_k)$，其中$v_k$是上一次迭代得到的值
  2. 值更新：$v_{k+1}=r\pi_{k+1}+\gamma P_{\pi_{k+1}}v_k$,策略是上面迭代得到的策略，求出$v_{k+1}$后继续用于下一次迭代
- 对于每次迭代的值$v_k$：它不是任何一个状态的状态值，尽管时其会收敛到最优状态值，但k有限时，$v_k$可能不满足任何一个贝尔曼方程，因此也不是某一个策略的状态值
- 伪代码

```cpp
初始化：已知模型：对任意(s,a)对应的p(r|s,a)和p(s'|s,a)，设定初始值v0
目标：求解最优状态值和最优策略

当v_k未收敛时，进行如下迭代：
	(遍历)对于每个状态s:
		(遍历)对于每个动作a
			计算动作值q;
		选择最大动作值的action;	
		策略更新：即π_{k+1}(a|s)=1; //迭代第一步
		值更新：v_{k+1}(s)=max q_k(s,a) //迭代第二步
```

### 2.策略迭代算法

- 第k次迭代的两个步骤
  1. **策略评价**（policy-evaluation)：评估上一次得到的策略$\pi _k$，评估方法就是求在当前状态下使用策略$\pi _k$得到的贝尔曼方程$v_{\pi_k}=r_{\pi_k}+\gamma P_{\pi_k}v_{\pi_k}$，从而得到$\pi _k$的状态值
  2. **策略改进**（policy improvement）：用来改进策略以得到更好的策略，方法是在第一步得到$v_{\pi_k}$后，利用$\pi_{k+1}=argmax_{\pi}(r_\pi + \gamma P_\pi v_{\pi_k})$
- 值迭代和策略迭代的区别
  - 值迭代：对于每个(s,a)，初始化一个v，每次选择q值最大的策略，使用这个策略去更新v，下一轮再拿更新后的v去找最优策略，即**反复使用贝尔曼最优方程去更新v**，每一步都隐含策略更新；
  - 策略迭代：对每个(s,a)，初始化一个策略π，先求在这个π下的状态值v来评估这个策略，再拿这个v来求最优策略（q值最大），（显示的交替进行PE和PI）
- 三个关键问题（这里看gpt学了，以后有需要再看书）
  1. 策略评价过程中，如何计算状态值$v_{\pi _k}$
     - 已知$\pi_k$，$p(r|s,a),p(s'|s,a)$，使用贝尔曼方程求解即可
     - 如何解贝尔曼方程：参考书上第二章求贝尔曼方程的方法，法一直接解矩阵方程，这个涉及到求逆矩阵，一般不使用；法二是**迭代法近似求解**，即初始化一个$v_0$，一步步迭代出最终的$v_{\pi_k}$
  2. 策略改进中，为什么新策略$\pi _{k+1}$比之前的$\pi _k$更好
     - 首先复习策略更好的定义：指的是对于**每一个状态**，该策略的状态值**都比**其余策略的状态值要大
     - 策略改进定理：如果$\pi_{k+1}=argmax_{\pi}(r_\pi + \gamma P_\pi v_\pi)$，则$v_{\pi_{k+1}}>=v_{\pi_k}$，人话：即选择当前状态值下动作值最大的action对应的策略，一定比之前的策略好
  3. 为什么这个算法可以最终收敛到最优策略
     - 这个直接类比**数列单调有界定理**
     - 首先策略是**有界的**，即对于有限个状态，有限个动作而言，策略的总数的有限的
     - 其次由第二点得知，每轮迭代都会使策略更好，可以理解为策略性能是**单调递增的**
     - 由单调有界定理得知，策略一定会收敛到一个最优策略

### 3.截断策略算法

- 值迭代 VS 策略迭代
  - 策略迭代：从一个策略$\pi _0$出发，求状态值$v$，再拿求到的$v$，去找最优策略（动作值最大）
  - 值迭代：从值$v_0$出发，先求在这个$v$的最优策略$\pi$，再拿$\pi$去更新$v$

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/35b9fcd1331500360b4ac7d0a544941d.png" alt="image-20250623100036683" style="zoom: 80%;" />	

![image-20250623100754935](https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/d03c806c210d34904716b92e2ec371b0.png)

- 由上图可以看到，如果将值迭代的Value初始值设为$v_{\pi _0}$（策略迭代由初始策略求出的状态值），那么两种方法求最优策略，以及求Value的方式都非常相似，求Policy就是算出动作值最大的Action，求Value就是贝尔曼方程
- 但实际在求Value的过程中，二者是有区别的
  - 值迭代的value，就是求一次贝尔曼方程，拿上一次的$v_k$来求本次的$v_{k+1}$
  - 策略迭代的Value，也是求贝尔曼方程，但是用迭代法去求（需要迭代多轮贝尔曼方程，最终收敛）

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/0e39d742100f06ea1202ff9f5cdb2015.png" alt="image-20250623101749962" style="zoom:67%;" />	

- 所以就会想有没有这样一个中间状态，在迭代法求贝尔曼方程时，规定只迭代到第j步，得到的$v$再用于下一步的迭代，这样的方法就是**截断策略迭代**，显然截断策略迭代是值迭代和策略迭代的一种一般形式，当$j = 1$时就是值迭代，当$j\rightarrow \infty$时就是策略迭代
- 在实际使用中，我们使用的策略迭代实际上都是截断策略迭代，因为不可能去计算无穷步，只能说到达某一步后，看前后两个$v$差别，如果差别很小则认定收敛，停止迭代

## 五、蒙特卡洛方法

> 第四章的值迭代和策略迭代都是基于模型的方法（model-based），本节将第一次介绍无模型的方法（model-free） - 基于蒙特卡洛的方法

### 1.一个例子🌰

- 问题描述：投掷硬币，设结果为X，若硬币为正面，则X = +1，若硬币为反面，则X = -1，求解多次投掷后X的期望（平均值）
- 方法一：基于模型的方法，我们**事先知道**投掷为正面的概率为0.5，投掷为反面的概率为0.5，可以简单算出期望为0
  - 问题：很多问题中，我们可能事先不知道概率分布，因此，model-free的方法出现了
- 方法二：model-free方法，思路也很简单，不知道概率分布，就先采样n次，求平均，算出概率分布；由**大数定律**可知，当n越大时，得到的概率分布就越准确

### 2.MC Basic - 最简单的基于蒙特卡洛的强化学习算法

- 基于第四章的策略迭代算法，回顾策略迭代，其步骤分为”策略评估“和”策略改进“两部分，其中策略改进，就是基于评估得来的状态值v，计算最优动作值，选出最优策略
- 而在求动作值时，我们之前的方法，是基于$q_{\pi_k}(s, a) = \sum_{r} p(r \mid s, a) r + \gamma \sum_{s'} p(s' \mid s, a) v_{\pi_k}(s')$，即当下即时奖励 + $\gamma$*未来奖励，而这种方法下，$p(r|s,a)$，即在当前状态下采取动作a得到的奖励r，是模型信息，我们现在要做model-free，自然不能使用这种方式计算
- 回到动作值最初的定义，即$q_{\pi_k}(s, a) = \mathbb{E}[G_t \mid S_t = s, A_t = a]$

> 定义：动作值定义为**一个状态采取一个动作之后**，然后按照策略$\pi$行动，获得的回报的期望值

- 核心思想：用**实战经验（完整轨迹）**来估计动作值，而不是用贝尔曼方程和环境模型

### 3.MC Exploring Starts算法



### 4.MC $\epsilon$-Grddy算法

