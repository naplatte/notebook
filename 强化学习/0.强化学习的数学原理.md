## 一、基本概念

- > 强化学习讨论的问题：一个智能体（**agent**）如何在一个复杂的环境（**environment**）中去极大化它所获得的奖励。通过感知**环境的状态**（**state**）对动作（**action**）的反应（**reward**），来指导更好的动作，从而获得最大的收益（**return**）。以上过程称为在交互中学习，这样的学习方法称为强化学习。
- 一个例子：机器人走网格

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/453549f5955797d8b71c581e44ff9d81.png" alt="image-20250415110836537" style="zoom:80%;" />	

- **state**：agent相对于environment的一个status，在上面的例子中，state就是指位置，使用s1-s9表示这些位置状态；**环境决定状态**

- **state space**（状态空间）：即所有状态的集合

- **Action**：agent在某一时刻根据当前state做出决策/选择的行为，在上面例子中，在一个位置状态下，可能有5个action（向上下左右走&原地不动）
- 离散的动作空间：动作的选择是可数的，比如这里的上下左右，常用算法：Q-learning

- 连续的动作空间：动作类型是一个实数区间（无限可能），比如手臂的力度
- **Action space of state**：当前状态下所有的action集合 - 每个状态都有一个action space

- **state transition**：agent在某一state下执行某种action，转移到另一个state
  - 如果action触碰到边界，则相当于agent原地不动
  - 如果action进入到forbidden area，有两种情况：①forbidden area可进入，则直接进入；②forbidden area不可进入，则相当于原地不动

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/d958291fea7edcd0be81c439ab31a136.png" alt="image-20250415110900821" style="zoom:67%;" />	

- **state transition probability**：借助条件概率去描述状态转移
- 比如当前在s1，执行动作a2后，若P(s2|s1,a2) = 1，则说明一定会到达s2
  - 若P(s2|s1,a2) = 0.5，P(s5|s1,a2) = 0.5，则说明有50%的概率到达s2，有50%的概率到达s5

- **policy(策略）**：根据当前state，决定采取什么action

- **Reward(奖励)**：环境给agent一个反馈，用来指导agent是不是做得好，比如机器人走到出口 + 1分，机器人撞墙 - 1分

- **Episode(回合)**：agent从起点出发，直到任务结束，称为一轮

- **Trajectory(轨迹)**：agent一轮行动中，形如**(状态，动作，奖励)**这样的**序列**

- **Return(回报)**：从当前开始，到终点累计的总奖励（Reward之和），强化学习的目标就是最大化期望的Return，Return可以用来评估Policy的好坏

- 模拟机器人寻路代码

## 二、状态值与贝尔曼方程 

> 本章需要搞明白一个核心概念和一个核心工具，核心概念：状态值，核心工具：贝尔曼方程；需要使用核心工具 - 贝尔曼方程 来分析核心概念 - 状态值

### 1.关于回报

- 首先复习一下Return，Return就是从当前开始，到终点累计的Reward之和，这个Return针对的是一条轨迹，所以说Return是重要的，Return可以用来评价策略的好坏

- 如何计算Return

  - 法一：根据定义：Return就是沿轨迹收集的Reward之和，依次计算每次行动的Reward然后相加即可

  - 法二：自举法，类似于一个循环递归，简单说就是**不同状态出发的Reward是彼此依赖的**，对于下面的例子v1 - v4都是未知的，四个未知数四个方程，联立即可解出所有Return

    <img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/50478589b148b7932de4c2f48e51a466.png" alt="image-20250606112204576" style="zoom:67%;" />23	

### 2.状态值

- 对于一条轨迹，我们可以根据定义得到折扣回报

​		<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/39ce7b9c96a497093bc64e2ca3319d27.png" alt="image-20250606135651511" style="zoom:67%;" />	

- 上文的Return针对的是一条轨迹，而在一个state下，可能会有多条轨迹可以到达终点，对于每一条轨迹，我们都可以计算出一个Return，这里就引出了状态值的概念
- 状态值：对于一个state，所有轨迹的Return的期望（平均），即为状态值，状态值依赖于策略，依赖于状态，但不依赖于时间
- 一条轨迹即对应一个策略，状态值就是当前状态下所有策略的Return的期望
- 不同的策略可能得到相同的state Value

### 3.贝尔曼方程

- 贝尔曼方程：描述了所有状态值之间的关系
- 推导过程：首先是折扣回报的推导，其等于当前即时奖励 + 未来奖励

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/890c5ec4c0c2712c8fc404b7a02fd4a2.png" alt="image-20250608132721268" style="zoom:80%;" />	

- 上文提到状态值就是所有轨迹Return的期望，因此借助期望引入状态值的推导

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/ae9faf4bf393cf5c140f340084cac873.png" alt="image-20250608133046416" style="zoom:80%;" />	

- 第一项为**即时奖励的期望值**，在状态s有很多action可以take，我们假设take a的概率是π，第一行可以以全概率公式的角度去理解，第二行就是期望的定义，$p(r|s,a)r$即在状态s下take a，获得奖励为r的概率，再乘r，累加就是一个算期望的过程，感觉也可以以全概率公式的角度去理解

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/51d6626e4a10ce1390b57b0fbced2c4f.png" alt="image-20250608133709273" style="zoom:80%;" />	

- 第二项为**未来奖励的期望**，由马尔科夫性，未来奖励进依赖于当前状态，与先前状态无关，故$S_{t}=s$可去，因为当前求的是t+1时刻的奖励，最外层的求和也去按全概率的角度理解，最后两行也不难理解

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/134ef5b7174acc5051ba953149ae804f.png" alt="image-20250608135643115" style="zoom:80%;" />	

- 最终形式：整合上面二者

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/22e6234afcdbf2fd691a741cb6463bfd.png" alt="image-20250608140207061" style="zoom:80%;" />	

- 矩阵向量形式：状态值 = 即时奖励 + 折扣因子 * 状态转移概率矩阵 * 状态值，直观上理解也是状态值 = 当前即时奖励 + 未来奖励，其中P是一个矩阵，状态值V是一个向量，PV相乘后的向量即为每个状态的未来价值期望

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/0425ee14abade8f93ceb9d9ea84710bc.png" alt="image-20250608142030360" style="zoom:80%;" />	

### 4.动作值

- 定义：动作值定义为**一个状态采取一个动作之后**获得的回报的期望值
- 对比状态值
  - 状态值是对当前状态下，多个策略（轨迹）得到的Return求期望 - **从当前状态s出发的长期收益**
  - 动作值是在当前状态下，选择不同action得到Return（折扣奖励）的期望 - **从当前状态s选择动作a后的长期收益**
- 计算：可由状态值公式表示

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/442df695974d02c9b7f23d5d4c592c9a.png" alt="image-20250608144001622" style="zoom:80%;" />	

- 即使一个动作不会被策略选择，其依然具有动作值
- 强化学习的目的是寻找最优策略，因此必须探索每个状态下的所有动态，找到最优动作

## 三、最优状态值与贝尔曼最优方程

> 核心概念：最优状态值，基于此可以**定义**最优策略；
>
> 核心工具：贝尔曼最优方程，基于此，可以**求解**最优状态值和最优策略

### 1.启发例子：策略是否可改进

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/5c84c4a3ec01f15257caf6d7b666d82c.png" alt="image-20250608151040101" style="zoom: 67%;" />	

- 显然在直观上，这个策略是有问题的，因为在s1时选择的action会进入forbidden area，利用上节介绍的动作值，我们也可以计算出，在s1状态时，向下移动对应的动作值是最大的
- 如果我们更新策略从而使选择具有最大动作值的动作，就有望得到一个更好的策略

### 2.最优策略与贝尔曼最优方程

- 最优策略：在每一个状态都有比其他策略更高的状态值

- 一个状态的最优价值 = 在遵循最优策略时所获得的期望回报
- 贝尔曼最优方程：一个方程有两个未知数，分别为最优状态值v(s)和最优策略π
  - 贝尔曼最优方程就是用来描述**最优策略下的价值函数应该满足的关系**
  - **最优策略π选择具有最大动作值的动作**，方程转换为一个未知数，进而可求出最优状态值
  

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/b84d8d2cee49a3a1913f689f0b6b3d31.png" alt="image-20250608161433833" style="zoom: 67%;" />	

- 矩阵向量形式：类似上一节的贝尔曼方程，贝尔曼最优方程也有对应的矩阵向量形式

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/912272295670dea6d3cfd47bb9d48ee0.png" alt="image-20250610093900279" style="zoom:67%;" />	

- 上面的$\max_{\pi}(r_\pi+\gamma P_\pi v)$可以表示为$f(v)$，如何理解
  - 本质：**对每个状态s，考虑所有可能的策略$\pi$，在当前状态值$v$的基础上，选择一个策略，使在这个策略下即时奖励r+未来回报期望$\gamma P_\pi v$最大**

> 按我的思路帮我捋一下：
> 首先贝尔曼方程，它是研究状态值的工具对吧，每个状态都有一个贝尔曼方程对吧，在某个状态下，其状态值 = 即时奖励 + 未来回报期望对吧✔️
>
> 然后是贝尔曼最优方程，它应该也是求一个状态值对吗，既然是状态值，也就是说每一个状态都有一个贝尔曼最优方程对吧，当前状态下，按照贝尔曼最优方程求出的状态值，应该是最优状态值是吗，这个最优状态值，是在当前状态下选择最优策略得出的是吗✔️但是这个方程有个问题，就是它有两个未知量（v和π）对吗，这个策略π，就是在当前状态选择最优动作值的action对吗✔️那这个策略π表示的是一个动作还是多个动作？如果是多个动作的话，当前状态只能求在当前状态的动作值吧，怎么去选以后的动作，如果是一个动作就没问题了，当我没说好了。**（⭐一个动作，在当前状态下选择动作值最大的动作，每个状态都如此，最终就是最优策略）**
>
> 再回到我上面写成f(v)这个问题，这里是针对的矩阵向量形式，我能不能先考虑常规形式，也就是在某一状态下，我输入一个状态值，然后寻找在所有策略下，这个状态值什么时候最大对吗？但是这个输入的状态值是什么？是一个随机值吗？我不太懂这个f(v)的过渡，按照贝尔曼方程，它的目的就是去求出在当前状态下的最优状态值，而转换为矩阵向量形式时，为什么就变成了输入一个状态值v，求一个最优状态值，我不明白**==（强化学习的迭代过程就是“猜”+“逼近”，在初始时我们不知道最优V，那就猜一个，然后反复去用贝尔曼最优方程去更新它，直到收敛，这个更新过程就是函数映射，即输入一个V，通过计算在哪个策略下V最大，得到的最大V再作为函数输入，直到V收敛到一个不动点V*）==**

### 3.如何求解f(v)

#### ①数学前提知识

- 不动点：对于$x$，如果有$f(x) = x$ ，则x被称为不动点，即自变量经过函数映射后依然等于它本身
  - 