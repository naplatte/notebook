https://bony-monitor-135.notion.site/17c6741053b180ee9d80cf39fb2dee0f?pvs=73

## 第一章 Python入门

[Python基础](https://www.notion.so/Python-1686741053b1806a8e70f82d9c6ec5ab?pvs=21)

## 第二章 感知机的实现

### 2.1门电路与感知机的实现

- 感知机接收多个输入信号，输出一个信号
- 信号只有0和1两种取值
- 相同构造的感知机，只需适当的调整参数（权重和阈值）的值，就可以表示不同的逻辑电路

```python
#Python实现与门
def AND(x1,x2):
    w1,w2,theta = 0.5,0.5,0.7
    tmp = x1*w1+x2*w2
    if tmp <= theta:
        return 0
    elif tmp > theta:
        return 1

print(AND(0,1))
print(AND(1,1))
```

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/6e30c635-6b48-4ff0-a974-86e2e81bb486/cbe94f13-ce5e-43c9-9d9b-08dadaba4c75/image.png)

- 偏置的值决定了**神经元被激活的容易程度**
- 权重w即是对应**输入对输出结果的影响程度**
- 权重w和偏置b的设置依据：符合其门电路的真值表，以与门为例，就需保证仅当两个输入都有效时，结果才是有效的，其他情况都是无效的，设置的w和b需要满足这个条件
- 使用权重和偏置(b)实现与门（控制台版）

```python
x = np.array([0,1]) #输入
w = np.array([0.5,0.5]) #权重
b = -0.7
w*x
Out[6]: array([0. , 0.5])
np.sum(w*x)
Out[7]: 0.5
np.sum(w*x) + b
Out[8]: -0.19999999999999996
```

- 使用权重和偏置实现与门

```python
def AND(x1,x2):
    x = np.array([x1,x2])
    w = np.array([0.5,0.5])
    b = -0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    return 1
```

- 实现与非门与或门（仅权重和偏置同与门不同）

```python
#实现与非门
def NAND(x1,x2):
    x = np.array([x1,x2])
    w = np.array([-0.5,-0.5])
    b = 0.7
    tmp = np.sum(w * x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
#实现或门
def OR(x1,x2):
    x = np.array([x1,x2])
    w = np.array([0.5,0.5])
    b = 0.2
    tmp = np.sum(w * x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

### 2.2感知机的局限性 & 多层感知机

- 只能表示由一条直线分割的空间 - 无法实现异或门
- 线性空间：由一条直线分割的空间
- 非线性空间：由曲线分割的空间
- 多层感知机实现异或门

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/6e30c635-6b48-4ff0-a974-86e2e81bb486/c7757e0e-e6eb-4da6-a322-5e27b9fd858c/image.png)

```python
def XOR(x1,x2):
    s1 = NAND(x1,x2)
    s2 = OR(x1,x2)
    y = AND(s1,s2)
    return y
```

### 2.3番外：理解偏置的作用

1. 调整神经元的激活阈值：如果没有偏置，所有神经元的激活边界都会固定在原点（输入为 0 时输出也必须是 0）。
2. 但有些模型输入为0时，输出不一定是0，例如输入一个学生的学习时间，预测他通过考试的概率，事实是，即使学习0小时，仍有小概率通过考试
3. 想b站视频的例子，斜率+截距才能更好的拟合曲线

## 第三章 神经网络

### 3.1激活函数

- 概念：将输入信号的总和转换为输出信号的函数
- 实现阶跃函数

```python
import numpy as np
import matplotlib.pyplot as plt

def step_function(x):
    return np.array(x > 0, dtype=np.integer)

x = np.arange(-5.0, 5.0, 0.1) #x的范围是从-5到5，以0.1为单位
y = step_function(x)
plt.plot(x,y)
plt.ylim(-0.1,1.1) #指定y轴范围
plt.show()
```

- 实现sigmoid函数

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x)) 
```

- 神经网络的激活函数必须使用**非线性函数，**这样才能发挥叠加层的优势，线性函数的叠加层仍然是线性函数，就失去了叠加层的意义
- ReLU函数：当输入大于0时，直接输出该值；小于0时输出0

```python
def ReLU(x):
    return np.maximun(x,0)
```

### 3.2神经网络的实现

- 矩阵乘法

```python
A = np.array([[1,2],[3,4],[5,6]]) #3*2矩阵
B = np.array([[1,2,3],[4,5,6]]) #2*3矩阵
print(np.dot(A,B))
```

- 实现忽略偏置和激活函数的神经网络

```python
X = np.array([1,2])
W = np.array([[1,3,5],[2,4,6]])
Y = np.dot(X,W)
print(Y)
```

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/6e30c635-6b48-4ff0-a974-86e2e81bb486/97b9df23-5387-4476-accd-d0a7b4e8a0fd/image.png)

- 实现上图输入层（第0层）到第一层的信号传递（随机权重和偏置值）

```python
X = np.array([1.0,0.5])
W1 = np.array([[0.1,0.3,0.5],
               [0.2,0.4,0.6]])
B1 = np.array([0.1,0.2,0.3])
A1 = np.dot(X,W1) + B1 #实现A = XW + b
Z1 = sigmoid(A1)
```

- 实现上图第一层到第二层的信号传递，即上层的输出（Z1）变为本层的输入

```python
W2 = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])
B2 = np.array([0.1,0.2])
A2 = np.dot(Z1,W2) + B2
Z2 = sigmoid(A2)
```

- 第二层到输出层的信号传递同理
- 整合代码，实现上图完整的神经网络

```python
def init_network():
    network = {} #字典变量
    network['W1'] = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])
    network['b1'] = np.array([0.1,0.2,0.3])
    network['W2'] = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])
    network['b2'] = np.array([0.1,0.2])
    network['W3'] = np.array([[0.1,0.3],[0.2,0.4]])
    network['b3'] = np.array([0.1,0.2])
    return network
def forword(network,x):
    W1,W2,W3 = network['W1'],network['W2'],network['W3']
    b1,b2,b3 = network['b1'],network['b2'],network['b3']
    a1 = np.dot(x,W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1,W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2,W3) + b3
    y = identity_function(a3)
    print(y)

network = init_network()
x = np.array([1.0,0.5])
y = forword(network,x)
print(y)
```

### 3.3输出层设计

- 分类问题：数据属于哪一个类别的问题
- 回归问题：根据输入预测一个数值
- 实现softmax函数

```python
def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y #exp_a,sum_exp_a以及y都是数组类型                                                                                                                                                                                                                                                                                                                                                                                                                                           
```

- **softmax的输出总和恒为1，正因如此，才可以将softmax的输出解释为概率**
- 输出层神经元数量：若是分类问题，则输出层神经元数量应和类比数量相同，eg文本情况四分类问题，则输出层神经元数量就应为4

### 3.4Demo：手写数字识别

- 展示图像

```python
import numpy as np
import sys, os
from mnist import load_mnist
from PIL import Image #提供图像处理相关操作

def img_show(img):
#将 img 先转换为 numpy 的 uint8 类型数组，然后使用 Image.fromarray() 方法将其转换为 PIL 的图像对象，并将结果存储在 pil_img 中
    pil_img =Image.fromarray(np.uint8(img))
    pil_img.show()

(x_train,t_train), (x_test,t_test) =load_mnist(flatten = True,normalize= False)
img = x_train[0]  #数组第一个元素
label =t_train[0]
print(label)

print(img.shape)
img =img.reshape(28,28)
print(img.shape)

img_show(img)
```

- 实战

```python
import sys, os
import numpy as np
import pickle
from mnist import load_mnist
from neural_network import sigmoid,softmax

def get_data():
    (x_train,t_train),(x_test,t_test) = load_mnist(normalize=True,flatten=True,one_hot_label=False)
    return x_test,t_test

#初始化神经网络
def init_network():
    with open("sample_weight.pkl","rb") as f:
        network = pickle.load(f)
    return network

#预测
def predict(network,x):
    W1,W2,W3 = network['W1'],network['W2'],network['W3']
    b1,b2,b3 = network['b1'],network['b2'],network['b3']

    a1 =np.dot(x,W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1,W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2,W3) + b3
    y = softmax(a3)

    return y #返回的y是长度为10的数组，y[i]表示预测数字为i的概率

#实例
x,t = get_data()
network = init_network()
accuracy_cnt = 0
for i in range(len(x)):
    y = predict(network,x[i])
    p = np.argmax(y) #获得概率最高的元素的索引
    if p ==t[i]:
        accuracy_cnt += 1
print("Accuracy:" + str(float(accuracy_cnt) / len(x)))
```

- 批处理：把原先一个数据的单位换成打包好的一批数据，代码先不敲了，以后遇到再回顾

## 第四章 神经网络的学习

### 4.1训练与学习

- 学习：从训练数据中自动获取最优权重参数的过程
- 学习的目的：以损失函数为基准，找出能使它的值达到最小的权重参数
- 数据分为训练数据和测试数据
  - 使用训练数据进行学习，寻找最优参数
  - 使用测试数据评价训练后模型的实际能力
- 机器学习的最终目标：获得泛化能力
- 泛化能力：处理未被观察过的数据（训练集以外的数据）
- 过拟合：只对某一个数据集过渡拟合的状态
- 监督数据
  - 监督数据是带有标签的数据集，用于训练监督学习模型。
  - 它包括输入数据（特征）和对应的标签（目标值）。
  - **模型在训练过程中比较监督数据的结果与其真实标签，调整参数以达到最小损失值**
  - 监督数据的质量直接影响模型的性能。

### 4.2损失函数

- 相关概念
  - 损失函数是表示神经网络“恶劣程度”的指标，即当前神经网络在对监督数据在多大程度上不拟合
  - 机器学习就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数
- 均方误差
  - 均方误差越小，输出结果与监督数据越吻合

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/6e30c635-6b48-4ff0-a974-86e2e81bb486/82cbff05-9a87-4165-be62-e29e0f79fdf8/image.png)

```python
def mean_squared_error(y,t): #y是输出，t是监督数据
    y = np.array(y)
    t = np.array(t)
    return 0.5 * np.sum((y-t) ** 2)
```

- 交叉熵误差
  - 交叉熵误差的值由正确解标签对应的输出结果决定（错误解t = 0，这一项式子就为0了）
  - 正确解标签对应的输出越大，交叉熵误差越接近0（最大就是概率为1，log1 = 0）
  - 正确解标签对应的输出越小，交叉熵越大（log0→负无穷，整体加负号就是越大咯）

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/6e30c635-6b48-4ff0-a974-86e2e81bb486/ba36adea-a53d-4312-ae9d-6c777eeb32a6/image.png)

```python
def cross_entropy_error(y,t):
    y = np.array(y)
    t = np.array(t)
    delta = 1e-7
    return -np.sum(t * np.log(y + delta)) #delta是一个很小的数，防止y->0时log->负无穷溢
```

- mini-batch学习

  - 因为计算损失函数需要全部训练数据参与，当训练量很大时，为了节省时间，往往抽一个mini-batch计算损失函数
  - 对于读数据集时one-hot

  ![image.png](attachment:e8680228-f59c-4bd4-a02a-4a72d770c3ba:image.png)

  - 读取数据集

  ```python
  (x_train, t_train),(x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
  print(x_train.shape) #训练数据（图像），60000个784维的向量(28*28)
  print(t_train.shape)#测试数据的标签10000个长度为10的向量（one-hot）
  ```

  - 随机抽取10笔数据

  ```python
  train_size = x_train.shape[0] #60000个训练量
  batch_size = 10
  batch_mask = np.random.choice(train_size,batch_size) #从 [0, train_size-1] 范围内随机选择 batch_size 个不重复的索引
  x_batch = x_train[batch_mask]
  t_batch = t_train[batch_mask]
  ```

- 损失函数的必要性

  - 目的：通过调参提高精度
  - 为什么不直接使用识别精度作为指标而引入损失函数：以识别精度作为指标，参数的导数在绝大多数地方都为0
    - 识别精度是个离散的指标，不可导
    - 识别精度对参数不敏感，例：当前预测[0.51,0.49]预测类别为1，稍微修改参数可能变为[0.49,0.51]预测类别为2，这种突变使得参数优化过程变得困难

### 4.3数值微分

- 是什么：用数值方法近似求解函数导数的过程
- 导数：使用中心差分可以减少数值差分的误差

```python
#中心差分
def numercia_diff(f,x):
    h = 1e-4
    return (f(x+h) - f(x-h)) / (2*h)
#示例函数
def func(x):
    return x**2 + 2*x

#绘制函数图像
x = np.arange(0.0,20.0,0.1)
y = func(x)
plt.xlabel("x")
plt.ylabel("y")
plt.plot(x,y)
plt.show()

#使用数值微分计算导数
print(numercia_diff(func,5)) #0.1999999999990898
#实际值为12，可见微分对比导数存在误差，但很小
```

### 4.4梯度

- 全部偏导数汇总而成的向量称为梯度
- 实现数值梯度计算

```python
def numerical_gradient(f,x):
    h = 1e-4
    grad = np.zeros_like(x) #生成与x形状相同的数组，用于存储梯度
    
    for idx in range(x.size):
        tmp_val = x[idx] #遍历数组x中的元素，并保存在tm_val中
        
        x[idx] = tmp_val + h
        fxh1 = f(x) #fxh1即为f(x + h)
        
        x[idx] = tmp_val - h
        fxh2 = f(x) #fxh2即为f(x - h)
        
        #计算梯度
        grad[idx] = (fxh1 - fxh2) / (2 * h)
        x[idx] = tmp_val #还原x[idx]的值
        
    return grad
```

- 性质
  - 梯度会指向各点处的函数值降低的地方，更严格的说：梯度指示的方向是各点处函数值减小最多的方向。
  - **梯度的方向是函数值增长最快的方向，而它的反方向是函数值减小最快的方向。**
- 梯度法
  - 梯度的方向不一定指向最小值，但**沿梯度的方向可以最大限度的减小函数的值**
  - 梯度法：函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方再求梯度，再沿着新梯度方向前进，如此往复
  - 寻找最小值：梯度下降
  - 寻址最大值：梯度上升
  - **梯度下降的目的是找最小值或极小值，通过调参找到损失函数的最小值点即可提高模型精度**
- 实现梯度下降
  - 像学习率这样的参数称为**超参数**，超参数需要**人工设定**

```python
def gradient_descent(f,init_x,lr=0.01,step_num=100):
    x = init_x

    for i in range(step_num):
        grad = numerical_gradient(f,x)
        x = x - lr * grad
    return x

#试验
init_x = np.array([-3.0,4.0])
print(gradient_descent(f,init_x=init_x,lr=0.1,step_num=100))
```

- 学习率过大或过小的情况

![image.png](attachment:8ad5d37d-4de6-482d-965b-6e7eb025693a:image.png)

### 4.5学习算法的实现

- 前提：神经网络存在合适的权值和偏置，调整权值和偏置以便拟合训练数据的过程称为“学习”
- “学习”的步骤
  1. 从训练数据中选择mini-batch，目标是减少mini-batch损失函数的值
  2. 为了减少mini-batch损失函数的值，需要求出各权重的梯度，梯度表示损失函数减小最快的方向
  3. 将权重沿梯度方向进行微笑更新（学习率）
  4. 重复1,2,3
- 实现上述步骤的二层神经网络

```python
import sys,os
import numpy as np
from loss_function import cross_entropy_error
sys.path.append(os.pardir)
from common.functions import *
from gradient import numerical_gradient

class TwoLayerNet:
    #三个size分别为输入层，隐藏层，输出层神经元数
    def __init__(self,input_size,hidden_size,
                 output_size,weight_init_std=0.01):
        #初始化权重和偏置
        #权重使用符合高斯分布的随机数进行初始化，偏置使用0进行初始化
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size,hidden_size)
        self.params['b1'] = np.zeros(hidden_size) #偏置b1和b2初始化为全0向量
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size,output_size)
        self.params['b2'] = np.zeros(output_size)

    def predict(self,x):
        W1,W2 = self.params['W1'],self.params['W2']
        b1,b2 = self.params['b1'],self.params['b2']
        a1 = np.dot(x,W1) + b1;
        z1 = softmax(a1)
        a2 = np.dot(z1,W2)
        y = softmax(a2)
        return y

    #计算损失函数
    #x为输入数据，t为监督数据
    def loss(self,x,t):
        y = self.predict(x)
        return cross_entropy_error(y,t)

    def accuracy(self,x,t):
        y = self.predict(x)
        y = np.argmax(y,axis=1) #预测结果
        t = np.argmax(t,axis=1) #真实结果

        accuracy = np.sum(y == t) / float(x.shape[0]) #用预测正确的样本数除以总样本数，得到准确率
        return accuracy

    #计算权重和偏置的梯度
    def numerical_gradient(self,x,t):
        loss_W = lambda W:self.loss(x,t)
        #梯度
        grads = {} #初始化一个空字典
        grads['W1'] = numerical_gradient(loss_W,self.params['W1']) #numerical_gradient(f,x)
        grads['b1'] = numerical_gradient(loss_W,self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W,self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W,self.params['b2'])
        return grads
```

- 学习算法的实现

```python
import numpy as np
from dataset.mnist import load_mnist
from TwoLayerNet import TwoLayerNet

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True,one_hot_label=True)

train_loss_list = []
train_acc_list = []
test_acc_list = []
#平均每个epoch的重复次数
iter_per_epoch = max(train_size / batch_size, 1)

#超参数
iters_num = 10000 #模型更新总轮数
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1
network = TwoLayerNet(input_size=784, hidden_size=50,output_size=10)

for i in range(iters_num):
    #获取mini-batch
    batch_mask = np.random.choice(train_size,batch_size) #从train_size中随机抽取 batch_size个索引，用于小批量梯度下降
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    #计算梯度
    grad = network.numerical_gradient(x_batch, t_batch)
    #更新参数
    for key in ('W1','b1','W2','b2'):
        network.params[key] -= learning_rate * grad[key]
    #记录学习过程
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)

 # 计算每个epoch的识别精度
 if i % iter_per_epoch == 0:
		 train_acc = network.accuracy(x_train, t_train)
		 test_acc = network.accuracy(x_test, t_test)
		 train_acc_list.append(train_acc)
		 test_acc_list.append(test_acc)
		 print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
```

## 第五章 误差反向传播法

### 5.1计算图

- 定义：将计算的过程用图形表示出来
- 正向传播：从左到右的计算，从计算图出发点到结束点的传播
- 反向传播：从右到左的计算，传播“局部导数”，将导数的值写在箭头下方
- 局部计算：各节点只需处理与自己有关的计算，类似流水线的一段
- 计算图可以保存局部计算的结果，通过传递计算结果，可以获得全局复杂计算的结果

### 5.2链式法则

- 详见《高等数学》多元函数

### 5.3反向传播

- 加法结点：梯度直接传递到输入
- 乘法结点：梯度乘以另一个输入的值（翻转值）
- 激活函数结点：梯度乘以激活函数的导数

### 5.4简单层实现

- 乘法层实现
- backward()的参数中需输入**关于正向传播时的输出变量的导数**

```python
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None

    def forward(self,x,y):
        self.x = x
        self.y = y
        out = x * y
        return out

    def backward(self,dout):
        dx = dout * self.y #翻转值
        dy = dout * self.x
        return dx, dy
```

- 乘法器应用-买苹果

```python
#买两个苹果
apple = 100
apple_num = 2
tax = 1.1
mul_apple_layer = MulLayer()
mul_tax_layer = MulLayer()
#前向传播
apple_price = mul_apple_layer.forward(apple,apple_num) #税前价格
price = mul_tax_layer.forward(apple_price,tax) #最终价格
print(price) #220

#反向传播
dprice = 1 #最后一层往前传播时，导数是1
dapple_price,dtax = mul_tax_layer.backward(dprice) #参数需输入正向传播时输出变量的导数
dapple,dapple_num = mul_apple_layer.backward(dapple_price)
print(dapple,dapple_num,dtax)
```

- 加法器实现

```python
class AddLayer:
    def __init__(self):
        pass
    def forward(self,x,y):
        out = x + y
        return out
    def backward(self,dout):
        dx = dout * 1
        dy = dout * 1
        return dx, dy
```

- 买两个苹果和三个橘子

```python
apple = 100
apple_num = 2
orange = 150
orange_num = 3
tax = 1.1
mul_apple_layer = MulLayer()
mul_orange_layer = MulLayer()
add_apple_orange_layer = AddLayer()
mul_tax_layer = MulLayer()
#前向传播
apple_price = mul_apple_layer.forward(apple,apple_num)
orange_price = mul_orange_layer.forward(orange,orange_num)
all_price = add_apple_orange_layer.forward(apple_price,orange_price)
price = mul_tax_layer.forward(all_price,tax)
#反向传播
dprice = 1
dall_price,dtax = mul_tax_layer.backward(dprice)
dapple_price,dorange_price = add_apple_orange_layer.backward(dall_price)
dorange,dorange_num = mul_orange_layer.backward(dorange_price)
dapple,dapple_num = mul_apple_layer.backward(dapple_price)
print(price)
print(dapple_num,dapple,dorange_num,dorange,dtax)
```

- **首先以从左到右的顺序调用正向传播，再以相反的顺序调用反向传播**

### 5.5激活函数层，Affine层，Softmax层的实现

- ReLu层：若正向传播x>0，则反向传播将上游值（右边）原封不动传给下游（左边）；若正向传播x≤0，则反向传播给下游的信号将停留在此处。

```python
class ReLu:
    def __init__(self):
        self.mask = None

    def forward(self, x):
        self.mask = (x <= 0) #mask为数组，数组内元素为bool型，标记哪些元素<=0
        out = x.copy() #out避免了直接在原数组x进行修改
        out[self.mask] = 0
        return out

    def backward(self, dout):
        dout[self.mask] = 0 你
        dx = dout
        return dx
```

- sigmoid层

![image.png](attachment:cdfdf1dc-d279-4323-9c01-be8689a5ea54:image.png)

```python
class sigmoid:
    def __init__(self):
        self.out = None

    def forward(self, x):
        out = 1 / 1 + np.exp(-x)
        self.out = out
        return out

    def backward(self, dout):
        dx = dout * (1 - self.out) * self.out
        return dx
```

- Affine层

  - 直观理解：实现XW + b的层
  - 矩阵乘积（dot结点）的反向传播可以通过乘法运算要求的矩阵形状反推
    - 另：各元素的形状应和L对其的偏导形状相同
    - 例：W应和L对W的偏导形状相同，X应和L对X的偏导形状相同
  - 批版本的Affine层：即有N个数据同时输入的Affine层
  - 实现

  ```python
  class Affine:
      def __init__(self,W,b):
          self.W = W
          self.b = b
          self.x = None
          self.dW = None
          self.db = None
  
      def forward(self, x):
          self.x = x
          out = np.dot(self.x, self.W) + self.b
          return out
  
      def backward(self, dout):
          dx = np.dot(dout * self.W.T)
          self.dW = np.dot(self.x.T,dout)
          self.db = np.sum(dout,axis=0)
          return dx
  ```

- softmax-with-loss层

```python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.y = None
        self.t = None

    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self,self.y,self.t) #计算损失值
        return self.loss

    def backward(self,dout = 1) :
        batch_size = self.t.shape[0]
        dx = (self.y - self.t) / batch_size
        return dx
```

### 5.6实现升级版二层神经网络

```python
import numpy as np
import sys,os
sys.path.append(os.pardir)
from ReLu import *
from collections import OrderedDict
from dataset.mnist import load_mnist

class TwoLayerNet:
    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):
        #初始化权重
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size,hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size,output_size)
        self.params['b2'] = np.zeros(output_size)
        #生成层
        self.layers = OrderedDict() #有序字典
        self.layers['Affine1'] = Affine(self.params['W1'],self.params['b1'])
        self.layers['ReLu1'] = ReLu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
        self.lastLayer = SoftmaxWithLoss()

    def predict(self,x):
        for layer in self.layers.values():
            x = layer.forward(x) #输入在每一层依次前向传播
        return x

    #x：输入数据，t：监督数据
    def loss(self,x,t):
        y = self.predict(x)
        return self.lastLayer.forward(y,t)

    def accuracy(self,x,t):
        y = self.predict(x)
        y = np.argmax(y,axis=1)
        if t.ndim != 1 :
            t = np.argmax(t,axis=1)
        accuracy = np.sum(y==t) / float(x.shape[0])
        return accuracy
 
    def gradient(self,x,t):
        #前向
        self.loss(x,t)
        #反向
        dout = 1
        dout = self.lastLayer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse() #反向传播与前向传播调用顺序相反
        for layer in layers:
            dout = layer.backward(dout) #依次传播各梯度

        grads = {}
        grads['W1'] = self.layers['Affine1'].dW
        grads['b1'] = self.layers['Affine1'].db
        grads['W2'] = self.layers['Affine2'].dW
        grads['b2'] = self.layers['Affine2'].db
        return grads
#图示
            输入数据 (x)
                 │
                 ▼
          ┌────────────┐
          │ Affine1    │   ← 线性变换： x * W1 + b1
          └────────────┘
                 │
                 ▼
          ┌────────────┐
          │ ReLU1      │   ← 非线性激活函数 (ReLU)
          └────────────┘
                 │
                 ▼
          ┌────────────┐
          │ Affine2    │   ← 线性变换： (ReLU1输出) * W2 + b2
          └────────────┘
                 │
                 ▼
     ┌────────────────────┐
     │ SoftmaxWithLoss    │   ← Softmax + 交叉熵损失
     └────────────────────┘
                 │
                 ▼
         输出预测 & 损失值
#使用该模型学习
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True,one_hot_label=True)
network = TwoLayerNet(input_size=784,hidden_size=50, output_size=10)
iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1
train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)
for i in range(iters_num):
    batch_mask = np.random.choice(train_size,batch_size) ##从train_size中随机抽取 batch_size个索引，用于小批量梯度下降
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    #求梯度
    grad = network.gradient(x_batch,t_batch)
    #更新参数
    for key in ('W1','b1','W2','b2'):
        network.params[key] -= learning_rate * grad[key]

    loss = network.loss(x_batch,t_batch)
    train_loss_list.append(loss)

    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train,t_train)
        test_acc = network.accuracy(x_test,t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print(train_acc,test_acc)
```

## 第六章 与学习相关的技巧

### 6.1参数的更新

- SGD（随机梯度下降法）的缺点

  - 如果函数的形状非均匀，搜索的路径就会非常低效
  - 低效的原因：梯度并非直接指向最小值

- Momentum

  - 实现

  ```python
  class Momentum:
      def __init__(self,lr=0.01,momentum=0.9):
          self.lr = lr
          self.momentum = momentum
          self.v = None
  
      def update(self,params,grads):
          if self.v is None: #第一次使用update，更新动量项为0
              self.v = {}
              for key,val in params.items():
                  self.v[key] = np.zeros_like(val)
  
          for key in params.keys():
              #更新动量项
              self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]
              #更新参数
              params[key] = self.v[key]
  ```

  - v是动量项，相当于是对梯度的加权平均，α表示对历史梯度的记忆程度，当α = 0时，就变成了SGD
  - 对比SGD的好处
    - SGD：像一个人在崎岖的山谷里走路，每次都根据当前的坡度调整方向，容易左右摇摆
    - Momentum：像一个滚动的球，之前的速度会影响未来的方向，所以走得更平稳，不容易被小的坡度变化干扰

  ![image.png](attachment:374e2cb6-843e-4322-8474-cc7746d53959:image.png)

- AdaGrad

  - 学习率衰减：随着学习的进行，学习率逐渐减小
  - AdaGrad：对每个参数定制学习率
  - h表示以前所有梯度的平方和，**参数的元素中变动较大（被大幅更新）的元素的学习率将变小**

  ![image.png](attachment:eaab26b2-e591-4eca-9738-76d70346ce35:image.png)

- Adam：上面二者的结合

### 6.2权重的初始值

- 为防止权重均一化，权重必须随机生成初始值，不能设定相同的值
- 设定合适的权重初始值，各层的激活值分布会有适当的广度，从而可以顺利进行学习
- 隐藏层的激活值分布
  - 权重使用标准差为1的高斯分布：梯度消失，原理看不懂，先放一下吧
  - 权重使用标准差为0.01的高斯分布：激活值在分布上有所偏向（不好，因为如果多个神经元都输出几乎相同的值，那只用一个神经元就好了呗）
  - 正解：使用“Xavier初始值“：如果前一层的节点数为n，则初始值使用标准差为1 / 根号下n的分布，这样前一层的节点数越多，要设定为目标节点的初始值的权重尺度就越小。
- **总结：当激活函数使用ReLU时，权重初始值使用He初始值，当激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值。**

### 6.3Batch Normalization

- 以进行学习时的mini-batch为单位，按mini-batch进行正规化（使数据分布的均值为0、方差为1）
- 将以上处理添加到激活函数的前面或后面，可以减小数据分布偏向（使激活值分布具有适当的广度），还是不知道为什么

### 6.4正则化

- 正则化：防止模型过拟合
- 过拟合原因
  - 模型拥有大量参数，表现力强
  - 训练数据少
- 法一：权值衰减，在学习的过程中对大的权重进行惩罚，来抑制过拟合
- 法二：Dropout：在学习的过程中随机删除神经元，训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递；测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出

### 6.5超参数的验证

- 评估超参数性能：不可以使用测试数据，防止超参数的值对测试数据发生过拟合
- 引入验证集：评估超参数的好坏
- 最简单获得验证集的方法：从训练数据中分割20%

```python
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

#打乱训练数据
x_train,t_train = np.random.shuffle(x_train,t_train)

#分割验证集
validation_rate = 0.2
validation_num = int(x_train.shape[0] * validation_rate)

x_val = x_train[:validation_num] #前num部分
t_val = t_train[:validation_num]
x_train = x_train[validation_num:] #剩余部分
t_train = t_train[validation_num:]
```

- 超参数最优化步骤
  1. 设定一个范围
  2. 在范围内随机采样
  3. 使用采样到的超参数学习，通过验证集评估模型识别精度（但要将epoch设置很小，缩短评估时间）
  4. 重复步骤2,3（100次左右），逐步缩小超参数范围

## 第七章 卷积神经网络

### 7.1整体结构

![image.png](attachment:b68e59fb-50b8-4227-bd0d-03eaf67baf66:image.png)

- 输出层仍是Affine+Softmax的组合
- 靠近输出层的层仍使用Affine + ReLu的组合
- 其余层conv+ReLu+(Polling)代替了之前的全连接层

### 7.2卷积层

- 全连接层的问题：数据的形状被忽视（eg：28*28的数据→784个数据的一维向量）

- 卷积层的输入数据：输入特征图；卷积层的输出数据：输出特征图

- 卷积运算

  - 输入数据经过滤波器，做**乘积累加运算**
  - 以上的运算得到的结果，加上**偏置，**得到最后的输出数据

  ![image.png](attachment:fb85e4d4-16c7-4a3a-af56-89694da3fdb6:image.png)

- 填充

  - 概念：向输入数据的周围填入固定的数据
  - 幅度为k的填充：用幅度为k像素（k个小方块）的0填充输入数据周围
  - 填充的目的：**调整输出大小**
  - 例：对大小为（4,4）的数据应用（3,3）的滤波器，输出会变成（2,2），即输出比输入减小了两个元素，若每次卷积操作都会缩小空间，在某时刻的输出空间就可以缩为1，导致无法再进行卷积运算
  - 应用填充后：（4,4）的输入填充为（6,6）（幅度为1的填充），经（3,3）的滤波器，输出数据仍为（4,4），达到卷积操作并未缩小空间的目的

- 步幅

  - 概念： 应用滤波器的位置间隔
  - 步幅增大，输出空间会减小
  - 填充增大，输出空间会增大

- 三维数据的卷积运算

  - 3维图像：高，长，通道
  - 运算方法和2维相似，只不过纵深方向上的特征图增加
  - **输入数据和滤波器的通道数要设为相同的值**

- 结合方块

  - 一般三维数据的书写顺序**（channel,height,width）即（C,H,W）**，对应滤波器（fliter）的书写形式（C,FH,FW）
  - 使用一个滤波器，只能输出一张特征图，若想在通道方向上拥有多个卷积运算的输出，则需要用到多个滤波器
  - 用FN的滤波器，输出特征图就会生成FN个，书写为（FN,OH,OW），滤波器书写为（个数，C，FH，FW）
  - 每个通道都有1个偏置

- 批处理

  - 需要在各层间传递的数据保存为4维数据
  - 在各个数据开头加上批用的维度（batch_num,C,H,W)

### 7.3池化层

- 概念：进行缩小高、长方向的空间的运算
- 核心思想：将输入的特征图中的相邻像素进行组合，以减少特征图的尺寸和计算量。
  - **作用：保留输入特征图中的主要信息，同时减少网络的参数数量和计算量。**
- Max池化：获取最大值，Average池化：获取平均值
- 池化的窗口大小一般和步幅设定为相同的值，eg2*2的窗口步幅设为2,3*3的窗口步幅设为3
- 池化层的特征
  - 没有要学习的参数
  - 通道（纵向深度）数不变
  - 输入数据微小变化，输出可能不变
- 卷积&池化
  - 卷积操作通过滤波器与输入特征图进行卷积，生成新的特征图。
  - 池化操作通过在输入特征图上移动池化窗口，选择值最大或平均的像素，生成新的特征图。

### 7.4卷积层和池化层的实现

- im2col：将输入数据重新排列，将其展开以适合滤波器，最终使整个卷积运算可以用一个大矩阵乘法完成。（现代GPU对矩阵乘法的优化非常好）
- 函数接口im2col(input_data,filter_h,fliter_w,stride,pad)，分别对应：输入数据（4维），滤波器的高，滤波器的长，步幅，填充

### 7.5CNN的实现

- 卷积层实现

```python
class Convolution:
    def __init__(self,W,b,stride=1,pad=0):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad

    def forward(self,x):
        FN,C,FH,FW = self.W.shape
        N,C,H,W = x.shape
        #计算输出特征图大小
        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)
        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)
        #进行卷积运算
        col = im2col(x,FH,FW,self.stride,self.pad)
        col_W = self.W.reshape(FN,-1).T #滤波器展开为1列
        out = np.dot(col,col_W) + self.b

        out = out.reshape(N,out_h,out_w,-1).transpose(0,3,1,2)
        return out
        
#反向传播的实现和bp基本相同，但需注意在进行卷积层的反向传播时，必须进行im2col逆处理
```

- 池化层实现

```python
class Pooling:
    def __init__(self,pool_h,pool_w,stride=1,pad=0):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad

    def forward(self,x):
        N, C, H, W = x.shape
        # 计算池化后的输出尺寸
        out_h = int(1 + (H - self.pool_h) / self.stride)
        out_w = int(1 + (W - self.pool_w) / self.stride)
        #展开
        col = im2col(x,self.pool_h,self.pool_w,self.stride,self.pad)
        col = col.reshape(-1,self.pool_h*self.pool_w)
        #最大值
        out = np.max(col,axis=1)
        #转换
        out = out.reshape(N,out_h,out_w,C).transpose(0,3,1,2)
        return out
```

### 7.6CNN的可视化

- 卷积层的滤波器会提取边缘或斑块等原始信息，CNN会将这些原始信息传递给后面的层
- 随着层次的加深，神经元从简单的形状向高级信息变化

### 7.7具有代表性的CNN

- LeNet：CNN元祖
- AlexNet

## 番外补充

### 1.梯度消失与梯度爆炸的原因及解决方案

- 梯度不稳定：在神经网络中的梯度是不稳定的，在靠近输入层的隐层中或会消失，或会爆炸
- 产生原因：前一层的梯度是由后面层的梯度的乘积构成，当存在**过多层**时，就会出现梯度不稳定的问题
- 梯度消失
  - 即梯度在传播过程中越来越小，最终及其微弱或接近0
  - 后果
    - 网络的**前面几层更新很慢**，几乎没有学习能力
    - 网络**难以捕捉低级特征**，影响整体性能
  - 产生原因
    - 激活函数选择不当：以sigmoid为例，σ(x) = 1 / 1 + exp(-x)，记y = σ(x)，不难求出其导数可以化简为y(1 - y)，而sigmoid值域为(0,1)，故当y→0时，导数→0，当y→1时，导数→0，造成梯度消失
    - 隐藏层过多：每一层都会将梯度乘以一个小于 1 的数，导致梯度指数级衰减。（sigmoid和tanh的导数最大值均<1）
  - 处理方法
    - 选择合适的激活函数：ReLU，正区间的梯度为1
    - 权重初始化：如 Xavier/Glorot 初始化、He 初始化，使得激活值在初始阶段不至于过饱和，缓解梯度消失问题
    - 批归一化（Batch Normalization）
- 梯度爆炸
  - 反向传播过程中，梯度值变得异常大，导致权重更新剧烈，引发数值不稳定甚至训练崩溃
  - 产生原因
    - 权重初始化不当（过高）（乘法结点：梯度乘以另一个输入的值（翻转值））
    - 网络结构不适合：eg时间序列很长的RNN
    - 学习率过高（如果学习率η 过大，梯度更新的步长就会过大，使得参数值大幅度变化，甚至可能跳过最优解，导致损失函数值不断增大，最终使训练无法收敛）
  - 处理方法
    - 梯度裁剪：在更新权重前，限制梯度的最大值
    - 降低学习率
    - 改进权重初始化
    - 使用更稳定的模型（LSTM）：这里去看循环神经网络的笔记

### 2.预训练

- 对于分类图片的预训练思想

  1. 通过网络上大量数据集训练出一个模型A

  2. 由于CNN的

     浅层

     学到的东西通用性很强，故对模型A做部分改进可得到模型B

     1. 冻结：浅层参数使用模型A的参数，深层参数随机初始化，浅层参数一直不变，然后利用现有数据集（少量）训练深层参数
     2. 微调：浅层参数使用模型A的参数，深层参数随机初始化，然后利用现有数据集训练参数，浅层参数也会随着训练发生变化

- **预训练的思想：想得到任务A对应的模型A，模型A的参数不是随机初始化的，而是通过任务B进行预先训练得到模型B，然后再利用模型B的参数对模型A进行初始化，再通过任务A的数据对模型A进行训练。**

### 3.机器学习 & 线性变换

![image.png](attachment:47df4931-991e-4601-a07c-9eb24627b5e1:image.png)