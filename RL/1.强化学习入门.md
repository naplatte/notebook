### 1.åŸºæœ¬æ¦‚å¿µ

- å¼ºåŒ–å­¦ä¹ è®¨è®ºçš„é—®é¢˜ï¼šä¸€ä¸ªæ™ºèƒ½ä½“ï¼ˆ**agent**ï¼‰å¦‚ä½•åœ¨ä¸€ä¸ªå¤æ‚çš„ç¯å¢ƒï¼ˆ**environment**ï¼‰ä¸­å»æå¤§åŒ–å®ƒæ‰€è·å¾—çš„å¥–åŠ±ã€‚é€šè¿‡æ„ŸçŸ¥**ç¯å¢ƒçš„çŠ¶æ€**ï¼ˆ**state**ï¼‰å¯¹åŠ¨ä½œï¼ˆ**action**ï¼‰çš„ååº”ï¼ˆ**reward**ï¼‰ï¼Œæ¥æŒ‡å¯¼æ›´å¥½çš„åŠ¨ä½œï¼Œä»è€Œè·å¾—æœ€å¤§çš„æ”¶ç›Šï¼ˆ**return**ï¼‰ã€‚ä»¥ä¸Šè¿‡ç¨‹ç§°ä¸ºåœ¨äº¤äº’ä¸­å­¦ä¹ ï¼Œè¿™æ ·çš„å­¦ä¹ æ–¹æ³•ç§°ä¸ºå¼ºåŒ–å­¦ä¹ ã€‚
- ä¸€ä¸ªä¾‹å­ï¼šæœºå™¨äººèµ°ç½‘æ ¼

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/453549f5955797d8b71c581e44ff9d81.png" alt="image-20250415110836537" style="zoom:80%;" />	

- **state**ï¼šagentç›¸å¯¹äºenvironmentçš„ä¸€ä¸ªstatusï¼Œåœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œstateå°±æ˜¯æŒ‡ä½ç½®ï¼Œä½¿ç”¨s1-s9è¡¨ç¤ºè¿™äº›ä½ç½®çŠ¶æ€ï¼›**ç¯å¢ƒå†³å®šçŠ¶æ€**

- **state space**ï¼ˆçŠ¶æ€ç©ºé—´ï¼‰ï¼šå³æ‰€æœ‰çŠ¶æ€çš„é›†åˆ

- **Action**ï¼šagentåœ¨æŸä¸€æ—¶åˆ»æ ¹æ®å½“å‰stateåšå‡ºå†³ç­–/é€‰æ‹©çš„è¡Œä¸ºï¼Œåœ¨ä¸Šé¢ä¾‹å­ä¸­ï¼Œåœ¨ä¸€ä¸ªä½ç½®çŠ¶æ€ä¸‹ï¼Œå¯èƒ½æœ‰5ä¸ªactionï¼ˆå‘ä¸Šä¸‹å·¦å³èµ°&åŸåœ°ä¸åŠ¨ï¼‰
- ç¦»æ•£çš„åŠ¨ä½œç©ºé—´ï¼šåŠ¨ä½œçš„é€‰æ‹©æ˜¯å¯æ•°çš„ï¼Œæ¯”å¦‚è¿™é‡Œçš„ä¸Šä¸‹å·¦å³ï¼Œå¸¸ç”¨ç®—æ³•ï¼šQ-learning

- è¿ç»­çš„åŠ¨ä½œç©ºé—´ï¼šåŠ¨ä½œç±»å‹æ˜¯ä¸€ä¸ªå®æ•°åŒºé—´ï¼ˆæ— é™å¯èƒ½ï¼‰ï¼Œæ¯”å¦‚æ‰‹è‡‚çš„åŠ›åº¦
- **Action space of state**ï¼šå½“å‰çŠ¶æ€ä¸‹æ‰€æœ‰çš„actioné›†åˆ - æ¯ä¸ªçŠ¶æ€éƒ½æœ‰ä¸€ä¸ªaction space

- **state transition**ï¼šagentåœ¨æŸä¸€stateä¸‹æ‰§è¡ŒæŸç§actionï¼Œè½¬ç§»åˆ°å¦ä¸€ä¸ªstate
  - å¦‚æœactionè§¦ç¢°åˆ°è¾¹ç•Œï¼Œåˆ™ç›¸å½“äºagentåŸåœ°ä¸åŠ¨
  - å¦‚æœactionè¿›å…¥åˆ°forbidden areaï¼Œæœ‰ä¸¤ç§æƒ…å†µï¼šâ‘ forbidden areaå¯è¿›å…¥ï¼Œåˆ™ç›´æ¥è¿›å…¥ï¼›â‘¡forbidden areaä¸å¯è¿›å…¥ï¼Œåˆ™ç›¸å½“äºåŸåœ°ä¸åŠ¨

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/d958291fea7edcd0be81c439ab31a136.png" alt="image-20250415110900821" style="zoom:67%;" />	

- **state transition probability**ï¼šå€ŸåŠ©æ¡ä»¶æ¦‚ç‡å»æè¿°çŠ¶æ€è½¬ç§»
- æ¯”å¦‚å½“å‰åœ¨s1ï¼Œæ‰§è¡ŒåŠ¨ä½œa2åï¼Œè‹¥P(s2|s1,a2) = 1ï¼Œåˆ™è¯´æ˜ä¸€å®šä¼šåˆ°è¾¾s2
  - è‹¥P(s2|s1,a2) = 0.5ï¼ŒP(s5|s1,a2) = 0.5ï¼Œåˆ™è¯´æ˜æœ‰50%çš„æ¦‚ç‡åˆ°è¾¾s2ï¼Œæœ‰50%çš„æ¦‚ç‡åˆ°è¾¾s5

- **policy(ç­–ç•¥ï¼‰**ï¼šæ ¹æ®å½“å‰stateï¼Œå†³å®šé‡‡å–ä»€ä¹ˆaction

- **Reward(å¥–åŠ±)**ï¼šç¯å¢ƒç»™agentä¸€ä¸ªåé¦ˆï¼Œç”¨æ¥æŒ‡å¯¼agentæ˜¯ä¸æ˜¯åšå¾—å¥½ï¼Œæ¯”å¦‚æœºå™¨äººèµ°åˆ°å‡ºå£ + 1åˆ†ï¼Œæœºå™¨äººæ’å¢™ - 1åˆ†

- **Episode(å›åˆ)**ï¼šagentä»èµ·ç‚¹å‡ºå‘ï¼Œç›´åˆ°ä»»åŠ¡ç»“æŸï¼Œç§°ä¸ºä¸€è½®

- **Trajectory(è½¨è¿¹)**ï¼šagentä¸€è½®è¡ŒåŠ¨ä¸­ï¼Œå½¢å¦‚**(çŠ¶æ€ï¼ŒåŠ¨ä½œï¼Œå¥–åŠ±)**è¿™æ ·çš„**åºåˆ—**

- **Return(å›æŠ¥)**ï¼šä»å½“å‰å¼€å§‹ï¼Œåˆ°ç»ˆç‚¹ç´¯è®¡çš„æ€»å¥–åŠ±ï¼ˆRewardä¹‹å’Œï¼‰ï¼Œå¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡å°±æ˜¯æœ€å¤§åŒ–æœŸæœ›çš„Returnï¼ŒReturnå¯ä»¥ç”¨æ¥è¯„ä¼°Policyçš„å¥½å

- æ¨¡æ‹Ÿæœºå™¨äººå¯»è·¯ä»£ç 

```python
import numpy as np
import time
class GridWorld:
    def __init__(self,size = 3):
        self.size = size
        self.start = (0,0)
        self.goal = (self.size - 1,self.size - 1)
        self.reset()

    def reset(self):
        self.state = self.start #å›åˆ°èµ·ç‚¹
        return self.state

    def step(self,action):
        x,y = self.state
        if action == "up" : x = max(0,x - 1)
        elif action == "down" : x = min(self.size - 1,x + 1)
        elif action == "left" : y = max(0,y - 1)
        elif action == "right" : y = min(self.size - 1,y + 1)

        self.state = (x,y)
        reward = 1.0 if self.state == self.goal else -0.1
        done = self.state == self.goal #åˆ¤æ–­æ¸¸æˆæ˜¯å¦ç»“æŸ
        time.sleep(1)  # æ¯æ­¥æš‚åœ 1 ç§’
        return self.state, reward,done

    def get_actions(self):
        return ["up","down","left","right"]

    def render(self):
        grid = [["â¬œ" for _ in range(self.size)] for _ in range(self.size)] #_ä»…è¡¨ç¤ºä¸€ä¸ªå ä½ç¬¦,gridæ˜¯ä¸€ä¸ªäºŒç»´åˆ—è¡¨
        x,y = self.state
        gx,gy = self.goal
        grid[gx][gy] = "ğŸ"
        grid[x][y] = "ğŸŸ©"
        for row in grid:
            print(" ".join(row)) #" ".join(row) ä¼šæŠŠè¿™å‡ ä¸ªå…ƒç´ ç”¨ç©ºæ ¼ " " è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²
        print() #æ‰“å°ä¸€ä¸ªç©ºè¡Œ

if __name__ == "__main__":
    env = GridWorld()
    state = env.reset()
    env.render() #ç”Ÿæˆç½‘æ ¼å›¾

    for _ in range (20):
        action = np.random.choice(env.get_actions())
        next_state,reward,done = env.step(action)
        print(f"Action:{action},Next state:{next_state},Reward:{reward}")
        env.render() #ç»˜åˆ¶æ­¥æ•°ä¹‹åçš„ç½‘æ ¼è¡¨
        if done:
            print("âœ”ï¸successful pass!")
            break
```

### 2.é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDP)ï¼ˆå‰æœŸä¹±å­¦è®°å½•ï¼Œç›´æ¥çœ‹è´å°”æ›¼å§ï¼‰

#### â‘ åŸºæœ¬æ¦‚å¿µ

- ä¸ºä»€ä¹ˆéœ€è¦MDPï¼šå¼ºåŒ–å­¦ä¹ ä¸­ï¼Œagentéœ€è¦åœ¨ä¸€ä¸ªç¯å¢ƒä¸­ä¸æ–­åšå†³ç­–ï¼Œè·å¾—å¥–åŠ±ï¼Œä»è€Œå­¦ä¹ â€œå¥½è¡Œä¸ºâ€ï¼Œä¸ºäº†ä¸¥è°¨çš„æè¿°`å†³ç­– - åé¦ˆ - å­¦ä¹ `çš„è¿‡ç¨‹ï¼Œå°±å¼•å…¥MDP

- MDPæ˜¯å¯¹**å®Œå…¨å¯è§‚æµ‹ç¯å¢ƒ**è¿›è¡Œæè¿°çš„ï¼Œè§‚æµ‹åˆ°çš„çŠ¶æ€å†…å®¹å®Œæ•´å†³å®šäº†å†³ç­–çš„éœ€è¦çš„ç‰¹å¾
  - ä½ ä¸éœ€è¦ä¾èµ–å†å²ä¿¡æ¯æˆ–è€…çŒœæµ‹éšè—å˜é‡â€”â€”å½“å‰çš„çŠ¶æ€å°±è¶³å¤Ÿåšå‡ºåˆç†çš„åŠ¨ä½œé€‰æ‹©(ä¸å…³å¿ƒã€Œæ€ä¹ˆæ¥åˆ°è¿™ä¸ªçŠ¶æ€ã€ï¼ˆæ²¡æœ‰â€œå‰åºâ€ï¼‰)
  - ä¾‹å¦‚æœºå™¨äººèµ°è¿·å®«å°±æ˜¯å®Œå…¨å¯è§‚æµ‹ï¼Œ**ä¸éœ€è¦**çŸ¥é“â€œä½ ä»å“ªå„¿æ¥â€ã€â€œä½ ä¹‹å‰èµ°äº†å‡ æ­¥â€â€”â€”å½“å‰çŠ¶æ€å·²ç»å¤Ÿç”¨äº†
- é©¬å°”ç§‘å¤«æ€§ï¼š**æœªæ¥åªä¾èµ–äºå½“å‰çŠ¶æ€å’Œå½“å‰åŠ¨ä½œ**ï¼Œä¸è¿‡å»æ— å…³ï¼Œå› ä¸ºå½“å‰çš„çŠ¶æ€å°±åŒ…å«äº†è¿‡å»çš„ä¿¡æ¯
  - ä½¿ç”¨**çŠ¶æ€è½¬ç§»æ¦‚ç‡**ï¼š$$p_{ss'} = P[S_{t + 1 = s'} | S_t = s]$$æè¿°é©¬å°”ç§‘å¤«æ€§ï¼Œå³åœ¨**å½“å‰çŠ¶æ€ä¸ºsçš„æ¡ä»¶ä¸‹ï¼Œä¸‹ä¸€æ—¶åˆ»çŠ¶æ€è½¬ç§»åˆ°sâ€™çš„æ¦‚ç‡**ï¼Œè¿™ä¸ªæ¦‚ç‡å°±è¢«ç§°ä¸ºçŠ¶æ€è½¬ç§»æ¦‚ç‡
    - $S_t$ä¸ºtæ—¶åˆ»çš„çŠ¶æ€
    - $P_{ss'}$ä¸ºå½“å‰çŠ¶æ€ä¸ºsï¼Œä¸‹ä¸€æ—¶åˆ»è½¬ç§»åˆ°sâ€˜çš„æ¦‚ç‡
  - **çŠ¶æ€è½¬ç§»çŸ©é˜µ**å®šä¹‰äº†æ‰€æœ‰çŠ¶æ€çš„è½¬ç§»æ¦‚ç‡ï¼ŒçŸ©é˜µä¸­æ¯è¡Œå…ƒç´ çš„å’Œä¸º1

$$
\begin{bmatrix}
	p_{11} ... p_{1n}\\
	\vdots \\
	p_{n1} ... p_{nn}
	
\end{bmatrix}
$$

- é©¬å°”ç§‘å¤«è¿‡ç¨‹ï¼šåˆç§°ä¸ºé©¬å°”ç§‘å¤«é“¾ï¼Œæ˜¯ä¸€ä¸ªæ— è®°å¿†çš„éšæœºè¿‡ç¨‹ï¼Œ*å¯ä»¥ç”¨ä¸€ä¸ªå…ƒç´ <S,P>è¡¨ç¤ºï¼Œå…¶ä¸­Sè¡¨ç¤ºçŠ¶æ€é›†ï¼ŒPè¡¨ç¤ºçŠ¶æ€è½¬ç§»çŸ©é˜µ*

- ç¤ºä¾‹ï¼šå­¦ç”Ÿé©¬å°”ç§‘å¤«é“¾ï¼šæ¯ä¸ªåœ†åœˆè¡¨ç¤ºçŠ¶æ€ï¼Œç®­å¤´è¡¨ç¤ºåœ¨å½“å‰çŠ¶æ€ä¸‹è½¬ç§»åˆ°ç›®æ ‡çŠ¶æ€çš„æ¦‚ç‡ï¼Œæ ¹æ®æ­¤å›¾ä¸éš¾ç”»å‡ºçŠ¶æ€è½¬ç§»çŸ©é˜µ

![img](https://raw.githubusercontent.com/mumushu1/Pictures/main/ee051a76fd0a8297dae1a74353dd4230.png)	

#### â‘¡é©¬å°”ç§‘å¤«å¥–åŠ±è¿‡ç¨‹ Markov Reward Process & å›æŠ¥Return

- å³åœ¨é©¬å°”ç§‘å¤«è¿‡ç¨‹çš„åŸºç¡€ä¸Šï¼Œå¢åŠ äº†**å¥–åŠ±R**å’Œ**è¡°å‡ç³»æ•°$\gamma$**ï¼Œæ•…å¯ä»¥ç”¨<S,P,R,$\gamma$>è¡¨ç¤º	
  - ==å¥–åŠ±å³å½“å‰æ—¶åˆ»tåœ¨çŠ¶æ€sä¸‹ï¼Œåœ¨ä¸‹ä¸€ä¸ªæ—¶åˆ»èƒ½è·å¾—çš„å¥–åŠ±æœŸæœ›==ï¼Œå³$$R_S = E[R_{t+1}|S_t = s]$$
  - è¡°å‡ç³»æ•°ï¼šæ˜¯ä¸€ä¸ª[0,1]çš„æ•°ï¼Œè¡¨ç¤º**æœªæ¥å¥–åŠ±çš„é‡è¦ç¨‹åº¦**
    - ä»€ä¹ˆæ˜¯æœªæ¥å¥–åŠ±ï¼šæ¯”å¦‚åšæ—¥ç»“ï¼Œæ¯å¤©100å…ƒï¼Œå…±7å¤©
    - ç°å®ç”Ÿæ´»ä¸­ï¼Œå¤§å¤šäººè®¤ä¸ºä»Šå¤©æ‹¿åˆ°çš„é’± > æœªæ¥æ‹¿åˆ°çš„é’±ï¼Œæ‰€ä»¥å¼•å…¥äº†è¡°å‡å› å­ï¼Œå¼±åŒ–å°†æ¥çš„å¥–åŠ±ï¼Œå³**è¶Šè¿œçš„å¥–åŠ±ï¼Œä»·å€¼è¶Šä½**
- å›æŠ¥Returnï¼šå³ä»å½“å‰æ—¶åˆ»tå¼€å§‹ï¼Œåˆ°ç»“æŸæ‰€æœ‰å¥–åŠ±çš„æ€»å’Œ

> è”ç³»ç¬¬ä¸€éƒ¨åˆ†çš„æ¦‚å¿µ
>
> **Reward(å¥–åŠ±)**ï¼šç¯å¢ƒç»™agentä¸€ä¸ªåé¦ˆï¼Œç”¨æ¥æŒ‡å¯¼agentæ˜¯ä¸æ˜¯åšå¾—å¥½ï¼Œæ¯”å¦‚æœºå™¨äººèµ°åˆ°å‡ºå£ + 1åˆ†ï¼Œæœºå™¨äººæ’å¢™ - 1åˆ†
>
> **Return(å›æŠ¥)**ï¼šä»å½“å‰å¼€å§‹ï¼Œåˆ°ç»ˆç‚¹ç´¯è®¡çš„æ€»å¥–åŠ±ï¼ˆRewardä¹‹å’Œï¼‰ï¼Œå¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡å°±æ˜¯æœ€å¤§åŒ–æœŸæœ›çš„Returnï¼ŒReturnå¯ä»¥ç”¨æ¥è¯„ä¼°Policyçš„å¥½å

- æœªæ¥å¥–åŠ±è®¡ç®— $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... \gamma^n R_{n+1}$$
  - $\gamma$åå‘0ï¼Œåˆ™è¡¨æ˜è¶‹å‘äºâ€œè¿‘è§†â€æ€§è¯„ä¼°
  - $\gamma$åå‘1ï¼Œåˆ™è¡¨æ˜åé‡è€ƒè™‘è¿œæœŸçš„åˆ©ç›Š
- å¯ä»¥æ¨å¯¼å¾—$G_t = R_{t+1} + \gamma G_{t+1}$ï¼ˆæä¸€ä¸ª$\gamma$å‡ºæ¥ï¼‰

#### â‘¢ä»·å€¼å‡½æ•° **Value Function**

- ä»·å€¼å‡½æ•°ç»™å‡ºæŸä¸€stateæˆ–actionçš„é•¿æœŸä»·å€¼
- æŸä¸€çŠ¶æ€çš„ä»·å€¼å‡½æ•°$$v(s) = E[G_t|S_t = s]$$ï¼Œå³ä»å½“å‰çŠ¶æ€å¼€å§‹ï¼Œé©¬å°”ç§‘å¤«é“¾**æ”¶è·(Return)**çš„æœŸæœ›ï¼ˆæœŸæœ›èƒ½è·å¾—çš„æœªæ¥æ‰€æœ‰å›æŠ¥çš„æœŸæœ›å€¼ï¼‰
- å½“$\gamma$ = 0æ—¶ï¼Œå„çŠ¶æ€çš„å³æ—¶å¥–åŠ±åŒå„çŠ¶æ€çš„ä»·å€¼ç›¸åŒï¼ˆä¹Ÿå°±æ˜¯ä¸Šé¢è¯´çš„è¿‘è§†ï¼‰
- å½“$\gamma$ != 0æ—¶ï¼Œå„çŠ¶æ€çš„ä»·å€¼å°±éœ€è¦é€šè¿‡è®¡ç®—å¾—åˆ°

![image-20250418075206069](https://raw.githubusercontent.com/mumushu1/Pictures/main/18bf7de43a6f062c518ef18da5f2589d.png)	

- ä¸Šå›¾æ˜¯$\gamma$ = 0.9çš„æƒ…å†µï¼Œå¯ä»¥ç®€å•ç†è§£ä¸€ä¸‹ï¼Œæ¯”å¦‚çŠ¶æ€ä¸ºclass3æ—¶ï¼Œå³æ—¶å¥–åŠ±R = -2ï¼Œä½†å› ä¸ºè¡°å‡å› å­ä¸º0.9ï¼Œå› æ­¤ä¼šåé‡è€ƒè™‘è¿œæœŸçš„åˆ©ç›Šï¼Œè€Œclass3æœ‰0.6çš„æ¦‚ç‡ä¼šè¿›å…¥passï¼Œè€Œpassçš„åŠæ—¶å¥–åŠ±ä¸º10ï¼Œæ•…ç»è®¡ç®—å¯å¾—class3çš„ä»·å€¼ä¸º4.1ï¼ˆæ„æ€å°±æ˜¯å› ä¸ºè€ƒè™‘äº†è¿œæœŸåˆ©ç›Šï¼Œclass3çš„ä»·å€¼ç›¸æ¯”å³æ—¶ä»·å€¼å¤§å¤§å¢åŠ ï¼‰
- Returnå’Œä»·å€¼çš„åŒºåˆ«
  - Returnæ˜¯å¯¹å•ä¸ªTrajectoryæ‰€æ±‚çš„
  - state Valueæ˜¯é’ˆå¯¹å¤šä¸ªTrajectoryçš„Returnï¼Œæ±‚è¿™äº›Returnçš„æœŸæœ›

#### â‘£è´å°”æ›¼å…¬å¼

- å°è¯•å¯¹ä¸Šè¿°çš„æŸä¸€çŠ¶æ€çš„ä»·å€¼å‡½æ•°è¿›è¡ŒåŒ–ç®€

$$
\begin{align}
	v(s) &= E[G_t|S_t = s]\\
	 	 &= E[R_{t+1} + \gamma R_{t+2} + \gamma ^2 R_[t+3] +...|S_t = s]\\
	 	 &= E[R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} +...)|S_t = s]\\
	 	 &= E[R_{t+1} + \gamma G_{t+1}|S_t = s]\\
	 	 &= E[R_{t+1} + \gamma v(S_{t+1})|S_t = s]
\end{align}
$$

- å‰é¢3è¡Œå°±æ˜¯é€’å½’ï¼Œé‡ç‚¹ç†è§£æœ€åä¸€è¡Œ$G_{t+1} = v(S_{t +1})$ï¼Œç”±ä¸Šè¿°å¯çŸ¥
  - $G_{t+1} = R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + ... \gamma^n R_{n+1}$ï¼Œè¡¨ç¤ºt+1æ—¶åˆ»å¼€å§‹ï¼Œæœªæ¥å¯è·å¾—çš„æ€»å¥–åŠ±ï¼Œä¹Ÿå°±æ˜¯æ”¶è·Return
  - $v(s_{t+1}) = E[G_{t+1}|S_{t+1} = s]$ï¼Œè¡¨ç¤ºt+1æ—¶åˆ»å¼€å§‹ï¼Œä¸åŒtrajectoryçš„Returnçš„æœŸæœ›
  - æš‚ä¸”ç†è§£ï¼šåœ¨ä½ è¿˜ä¸çŸ¥é“å°†æ¥ä¼šèµ°åˆ°å“ªä¸ªçŠ¶æ€çš„æ—¶å€™ï¼Œå°±å¯ä»¥ç”¨å¯¹ $S_{t+1}$ çš„æœŸæœ›æ¥è¿‘ä¼¼$G_{t+1}$
- æ‰€ä»¥ä»·å€¼å‡½æ•°å¯ä»¥æ‹†è§£ä¸º$$v(s) = E[R_{t+1}|S_t = s] + E[\gamma v(S_{t+1})|S_t = s)]$$
  - å‰ä¸€éƒ¨åˆ†ä¸ºå½“å‰æ—¶åˆ»çš„å³æ—¶å¥–åŠ±
  - åä¸€éƒ¨åˆ†ä¸º**ä¸‹ä¸€æ—¶åˆ»çŠ¶æ€çš„ä»·å€¼æœŸæœ›**
- å› æ­¤è´å°”æ›¼æ–¹ç¨‹æä¾›äº†ä¸€ç§é€’å½’çš„æ–¹å¼æ¥è®¡ç®—æ¯ä¸ªçŠ¶æ€çš„ä»·å€¼
- ä»·å€¼è¿­ä»£ï¼šåŸºäºåŠ¨æ€è§„åˆ’ï¼Œåå¤åº”ç”¨è´å°”æ›¼æ–¹ç¨‹æ¥è®¡ç®—æ¯ä¸ªçŠ¶æ€çš„æœ€ä¼˜ä»·å€¼
  - ç­–ç•¥è¯„ä¼°ï¼šç»™å®šä¸€ä¸ªç­–ç•¥ï¼Œè®¡ç®—æ¯ä¸ªçŠ¶æ€çš„ä»·å€¼
  - ç­–ç•¥æ”¹è¿›ï¼šåœ¨è¯„ä¼°ç­–ç•¥ä¹‹åï¼Œæ›´æ–°ç­–ç•¥ä¸ºå½“å‰çŠ¶æ€ä¸‹é€‰æ‹©ä»·å€¼æœ€å¤§çš„action

- å®ç°ä»·å€¼è¿­ä»£ï¼šåœ¨ `GridWorld` ä¸­è®¡ç®—æ¯ä¸ªä½ç½®çš„ä»·å€¼å¹¶æœ€ç»ˆå¾—åˆ°æœ€ä¼˜ç­–ç•¥ã€‚

```python
def value_iteration(env,gamma = 0.9,threshold = 0.01):
    value_table = np.zeros((env.size,env.size)) #çŠ¶æ€ä»·å€¼è¡¨
    while (1):
        delta = 0
        for x in range(env.size):
            for y in range(env.size):
                state = (x,y)
                if state == env.goal:
                    continue
                action_values = [] #åŠ¨ä½œä»·å€¼
                for action in env.get_actions():
                    next_state,reward,done = env.step(state,action)
                    action_value = reward + gamma * value_table[next_state[0],next_state[1]] #è´å°”æ›¼å…¬å¼
                    action_values.append(action_value)
                max_action_value = max(action_values)
                delta = max(delta, abs(value_table[x, y] - max_action_value)) #ä»·å€¼æ›´æ–°çš„å·®å€¼
                value_table[x, y] = max_action_value
        if delta < threshold:#å¦‚æœä¸€æ¬¡è¿­ä»£ä¸­ï¼Œæ‰€æœ‰çŠ¶æ€çš„ä»·å€¼å˜åŒ–éƒ½å¾ˆå°ï¼ˆå°äº thresholdï¼‰ï¼Œå°±è®¤ä¸ºå·²ç»â€œæ”¶æ•›â€äº†ï¼Œä¹Ÿå°±æ˜¯è¯´å·®ä¸å¤šå·²ç»æ˜¯æœ€ä¼˜è§£äº†ï¼Œå°±å¯ä»¥ åœæ­¢è¿­ä»£ã€‚
            break
    return value_table

if __name__ == "__main__":
    env = GridWorld()
    values = value_iteration(env)
    print("æœ€ç»ˆçŠ¶æ€ä»·å€¼è¡¨ï¼š")
    print(np.round(values, 2))
```

#### â‘¤é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹

- å°±æ˜¯åœ¨é©¬å°”ç§‘å¤«å¥–åŠ±è¿‡ç¨‹çš„åŸºç¡€ä¸ŠåŠ ä¸€ä¸ª`decision`è¿‡ç¨‹ï¼Œå¯¹æ¯”å¥–åŠ±è¿‡ç¨‹å¤šäº†ä¸€ä¸ª**åŠ¨ä½œé›†åˆ**ï¼Œä½¿ç”¨<S,A,P,R,$\gamma$>æ¥è¡¨ç¤º
  - S - çŠ¶æ€ç©ºé—´
  - A - åŠ¨ä½œç©ºé—´
  - P - çŠ¶æ€è½¬ç§»æ¦‚ç‡ï¼Œ$P_{ss'}$ä¸ºå½“å‰çŠ¶æ€ä¸ºsï¼Œä¸‹ä¸€æ—¶åˆ»è½¬ç§»åˆ°sâ€˜çš„æ¦‚ç‡
  - R - å¥–åŠ±å‡½æ•°Reward
  - $\gamma$ - è¡°å‡å› å­
- ç­–ç•¥ï¼ˆpolicyï¼‰ï¼šä½¿ç”¨Ï€è¡¨ç¤ºç­–ç•¥çš„é›†åˆï¼Œ`Ï€(a|s)`è¡¨ç¤ºçŠ¶æ€ä¸º`s`æ—¶ï¼Œé‡‡å–è¡ŒåŠ¨`a`çš„æ¦‚ç‡
- ç›®æ ‡ï¼šæ‰¾åˆ°ä¸€ä¸ªç­–ç•¥Ï€ï¼Œä½¿å¾—é•¿æœŸç´¯ç§¯å¥–åŠ±ï¼ˆReturnï¼‰æœ€å¤§åŒ– - å³ä¸Šé¢çš„$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... \gamma^n R_{n+1}$

- **ç­–ç•¥æå–ï¼šå¯¹æ¯ä¸ªçŠ¶æ€ï¼Œå°è¯•æ‰€æœ‰åŠ¨ä½œï¼Œè®¡ç®—æœ€ä¼˜ä»·å€¼**

```python
#æå–æ¯ä¸ªçŠ¶æ€çš„æœ€ä¼˜åŠ¨ä½œï¼ˆå³ç­–ç•¥ï¼‰
def extract_policy(value_table,env,gamma = 0.9):
    policy = np.full((env.size,env.size)," ",dtype=object) #å­˜å‚¨æœ€ä¼˜ç­–ç•¥
    for x in range(env.size):
        for y in range(env.size):
            state = (x,y)
            if state == env.goal:
                continue
            action_values = []
            for action in env.get_actions():
                next_state,reward,done = env.step(state,action)
                action_value = reward + gamma * value_table[next_state[0], next_state[1]]
                action_values.append(action_value)
            best_action = env.get_actions()[np.argmax(action_values)] #action_valuesæ˜¯ä»·å€¼åˆ—è¡¨ï¼Œå–ä»·å€¼æœ€å¤§çš„actionçš„åºå·iï¼Œget_actions()[i]å¯¹åº”çš„åŠ¨ä½œå°±æ˜¯æœ€ä¼˜åŠ¨ä½œ
            policy[x,y] = best_action
    return policy
```

- å¯è§†åŒ– + agentåº”ç”¨æœ€ä¼˜ç­–ç•¥

```python
#æ‰“å°æœ€ä¼˜ç­–ç•¥
def render_policy(policy, env):
    arrow_map = {
        "up": "â¬†",
        "down": "â¬‡",
        "left": "â¬…",
        "right": "â¡",
        " ": " "  # ç©ºç™½ç”¨äºç›®æ ‡çŠ¶æ€
    }
    print("æœ€ä¼˜ç­–ç•¥å›¾ï¼š")
    for x in range(env.size):
        row = ""
        for y in range(env.size):
            if (x, y) == env.goal:
                row += "ğŸ "
            else:
                row += arrow_map[policy[x, y]] + " "
        print(row)

#æœ€ä¼˜ç­–ç•¥åº”ç”¨åˆ°agent
def run_agent(env,policy):
    state = env.reset()
    env.render()
    steps = 0
    while state != env.goal:
        action = policy[state]
        state, reward, done = env.step(state,action)
        env.render()
        steps += 1
        if done:
            print("âœ”ï¸succesful pass!")
            break
```

