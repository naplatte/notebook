## 一、基本概念

- > 强化学习讨论的问题：一个智能体（**agent**）如何在一个复杂的环境（**environment**）中去极大化它所获得的奖励。通过感知**环境的状态**（**state**）对动作（**action**）的反应（**reward**），来指导更好的动作，从而获得最大的收益（**return**）。以上过程称为在交互中学习，这样的学习方法称为强化学习。

- 一个例子：机器人走网格

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/453549f5955797d8b71c581e44ff9d81.png" alt="image-20250415110836537" style="zoom:80%;" />	

- **state**：agent相对于environment的一个status，在上面的例子中，state就是指位置，使用s1-s9表示这些位置状态；**环境决定状态**

- **state space**（状态空间）：即所有状态的集合

- **Action**：agent在某一时刻根据当前state做出决策/选择的行为，在上面例子中，在一个位置状态下，可能有5个action（向上下左右走&原地不动）
- 离散的动作空间：动作的选择是可数的，比如这里的上下左右，常用算法：Q-learning

- 连续的动作空间：动作类型是一个实数区间（无限可能），比如手臂的力度
- **Action space of state**：当前状态下所有的action集合 - 每个状态都有一个action space

- **state transition**：agent在某一state下执行某种action，转移到另一个state
  - 如果action触碰到边界，则相当于agent原地不动
  - 如果action进入到forbidden area，有两种情况：①forbidden area可进入，则直接进入；②forbidden area不可进入，则相当于原地不动

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/d958291fea7edcd0be81c439ab31a136.png" alt="image-20250415110900821" style="zoom:67%;" />	

- **state transition probability**：借助条件概率去描述状态转移
- 比如当前在s1，执行动作a2后，若P(s2|s1,a2) = 1，则说明一定会到达s2
  - 若P(s2|s1,a2) = 0.5，P(s5|s1,a2) = 0.5，则说明有50%的概率到达s2，有50%的概率到达s5
- **policy(策略）**：根据当前state，决定采取什么action
- **Reward(奖励)**：环境给agent一个反馈，用来指导agent是不是做得好，比如机器人走到出口 + 1分，机器人撞墙 - 1分
- **Episode(回合)**：agent从起点出发，直到任务结束，称为一轮，一个episode是一条完整的trajectory，**从起始状态开始，到终止状态结束**
- **Trajectory(轨迹)**：agent一轮行动中，形如**(状态，动作，奖励)**这样的**序列**
- **Return(回报)**：从当前开始，到终点累计的总奖励（Reward之和），强化学习的目标就是最大化期望的Return，Return可以用来评估Policy的好坏

## 二、状态值与贝尔曼方程 

> 本章需要搞明白一个核心概念和一个核心工具，核心概念：状态值，核心工具：贝尔曼方程；需要使用核心工具 - 贝尔曼方程 来分析核心概念 - 状态值

### 1.关于回报

- 首先复习一下Return，Return就是从当前开始，到终点累计的Reward之和，这个Return针对的是一条轨迹，所以说Return是重要的，Return可以用来评价策略的好坏

- 如何计算Return

  - 法一：根据定义：Return就是沿轨迹收集的Reward之和，依次计算每次行动的Reward然后相加即可

  - 法二：自举法，类似于一个循环递归，简单说就是**不同状态出发的Reward是彼此依赖的**，对于下面的例子v1 - v4都是未知的，四个未知数四个方程，联立即可解出所有Return

    <img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/50478589b148b7932de4c2f48e51a466.png" alt="image-20250606112204576" style="zoom:67%;" />23	

### 2.状态值

- 对于一条轨迹，我们可以根据定义得到折扣回报

​		<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/39ce7b9c96a497093bc64e2ca3319d27.png" alt="image-20250606135651511" style="zoom:67%;" />	

- 上文的Return针对的是一条轨迹，而在一个state下，可能会有多条轨迹可以到达终点，对于每一条轨迹，我们都可以计算出一个Return，这里就引出了状态值的概念
- 状态值：对于一个state，所有轨迹的Return的期望（平均），即为状态值，状态值依赖于策略，依赖于状态，但不依赖于时间
- 状态值：在策略π下，从当前状态出发，未来所有可能轨迹回报的期望
- 不同的策略可能得到相同的state Value

### 3.贝尔曼方程

- 贝尔曼方程：描述了所有状态值之间的关系
- 推导过程：首先是折扣回报的推导，其等于当前即时奖励 + 未来奖励

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/890c5ec4c0c2712c8fc404b7a02fd4a2.png" alt="image-20250608132721268" style="zoom:80%;" />	

- 上文提到状态值就是所有轨迹Return的期望，因此借助期望引入状态值的推导

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/ae9faf4bf393cf5c140f340084cac873.png" alt="image-20250608133046416" style="zoom:80%;" />	

- 第一项为**即时奖励的期望值**，在状态s有很多action可以take，我们假设take a的概率是π，第一行可以以全概率公式的角度去理解，第二行就是期望的定义，$p(r|s,a)r$即在状态s下take a，获得奖励为r的概率，再乘r，累加就是一个算期望的过程，感觉也可以以全概率公式的角度去理解

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/51d6626e4a10ce1390b57b0fbced2c4f.png" alt="image-20250608133709273" style="zoom:80%;" />	

- 第二项为**未来奖励的期望**，由马尔科夫性，未来奖励进依赖于当前状态，与先前状态无关，故$S_{t}=s$可去，因为当前求的是t+1时刻的奖励，最外层的求和也去按全概率的角度理解，最后两行也不难理解

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/134ef5b7174acc5051ba953149ae804f.png" alt="image-20250608135643115" style="zoom:80%;" />	

- 最终形式：整合上面二者

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/22e6234afcdbf2fd691a741cb6463bfd.png" alt="image-20250608140207061" style="zoom:80%;" />	

- 矩阵向量形式：状态值 = 即时奖励 + 折扣因子 * 状态转移概率矩阵 * 状态值，直观上理解也是状态值 = 当前即时奖励 + 未来奖励，其中P是一个矩阵，状态值V是一个向量，PV相乘后的向量即为每个状态的未来价值期望

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/0425ee14abade8f93ceb9d9ea84710bc.png" alt="image-20250608142030360" style="zoom:80%;" />	

### 4.动作值

- 定义：动作值定义为**一个状态采取一个动作之后**获得的回报的期望值
- 对比状态值
  - 状态值是对当前状态下，多个策略（轨迹）得到的Return求期望 - **从当前状态s出发的长期收益**（关注从这里开始后能赚多少钱）
  - 动作值是在当前状态下，选择不同action得到Return（折扣奖励）的期望 - **从当前状态s选择动作a后的长期收益**（关注从这里出发，如果先选这个动作，再继续执行策略，可以赚多少钱）
  - **状态值更适合做“全局评价”，动作值更适合“决策和学习”**
- 计算：可由状态值公式表示

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/442df695974d02c9b7f23d5d4c592c9a.png" alt="image-20250608144001622" style="zoom:80%;" />	

- 即使一个动作不会被策略选择，其依然具有动作值
- 强化学习的目的是寻找最优策略，因此必须探索每个状态下的所有动态，找到最优动作

## 三、最优状态值与贝尔曼最优方程

> 核心概念：最优状态值，基于此可以**定义**最优策略；
>
> 核心工具：贝尔曼最优方程，基于此，可以**求解**最优状态值和最优策略

### 1.启发例子：策略是否可改进

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/5c84c4a3ec01f15257caf6d7b666d82c.png" alt="image-20250608151040101" style="zoom: 67%;" />	

- 显然在直观上，这个策略是有问题的，因为在s1时选择的action会进入forbidden area，利用上节介绍的动作值，我们也可以计算出，在s1状态时，向下移动对应的动作值是最大的
- 如果我们更新策略从而使选择具有最大动作值的动作，就有望得到一个更好的策略

### 2.最优策略与贝尔曼最优方程

- 最优策略：在每一个状态都有比其他策略更高的状态值

- 一个状态的最优价值 = 在遵循最优策略时所获得的期望回报
- 贝尔曼最优方程：一个方程有两个未知数，分别为最优状态值v(s)和最优策略π
  - 贝尔曼最优方程就是用来描述**最优策略下的价值函数应该满足的关系**
  - **最优策略π选择具有最大动作值的动作**，方程转换为一个未知数，进而可求出最优状态值

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/b84d8d2cee49a3a1913f689f0b6b3d31.png" alt="image-20250608161433833" style="zoom: 67%;" />	

- 矩阵向量形式：类似上一节的贝尔曼方程，贝尔曼最优方程也有对应的矩阵向量形式

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/912272295670dea6d3cfd47bb9d48ee0.png" alt="image-20250610093900279" style="zoom:67%;" />	

- 上面的$\max_{\pi}(r_\pi+\gamma P_\pi v)$可以表示为$f(v)$，如何理解
  - 本质：**对每个状态s，考虑所有可能的策略$\pi$，在当前状态值$v$的基础上，选择一个策略，使在这个策略下即时奖励r+未来回报期望$\gamma P_\pi v$最大**

> 按我的思路帮我捋一下：
> 首先贝尔曼方程，它是研究状态值的工具对吧，每个状态都有一个贝尔曼方程对吧，在某个状态下，其状态值 = 即时奖励 + 未来回报期望对吧✔️
>
> 然后是贝尔曼最优方程，它应该也是求一个状态值对吗，既然是状态值，也就是说每一个状态都有一个贝尔曼最优方程对吧，当前状态下，按照贝尔曼最优方程求出的状态值，应该是最优状态值是吗，这个最优状态值，是在当前状态下选择最优策略得出的是吗✔️但是这个方程有个问题，就是它有两个未知量（v和π）对吗，这个策略π，就是在当前状态选择最优动作值的action对吗✔️那这个策略π表示的是一个动作还是多个动作？如果是多个动作的话，当前状态只能求在当前状态的动作值吧，怎么去选以后的动作，如果是一个动作就没问题了，当我没说好了。**（⭐一个动作，在当前状态下选择动作值最大的动作，每个状态都如此，最终就是最优策略）**
>
> 再回到我上面写成f(v)这个问题，这里是针对的矩阵向量形式，我能不能先考虑常规形式，也就是在某一状态下，我输入一个状态值，然后寻找在所有策略下，这个状态值什么时候最大对吗？但是这个输入的状态值是什么？是一个随机值吗？我不太懂这个f(v)的过渡，按照贝尔曼方程，它的目的就是去求出在当前状态下的最优状态值，而转换为矩阵向量形式时，为什么就变成了输入一个状态值v，求一个最优状态值，我不明白**==（强化学习的迭代过程就是“猜”+“逼近”，在初始时我们不知道最优V，那就猜一个，然后反复去用贝尔曼最优方程去更新它，直到收敛，这个更新过程就是函数映射，即输入一个V，通过计算在哪个策略下V最大，得到的最大V再作为函数输入，直到V收敛到一个不动点V*）==**

### 3.如何求解f(v)

#### ①数学前提知识

- 不动点：对于$x$，如果有$f(x) = x$ ，则x被称为不动点，即自变量经过函数映射后依然等于它本身
- 压缩映射：$$||f(x_1) - f(x_2)|| <= \gamma ||x_1 - x_2||$$，直观理解就是，两点函数值的差<=其本身的差
- 压缩映射理论：对于方程$x = f(x)$，其中x和f(x)是实数向量，如果$f$是一个压缩映射，则下面性质成立
  - 存在性：必存在一个不动点$x^*$使$f(x^*)=x^*$
  - 唯一性：上面的$x^*$是唯一的
  - 如何求这个不动点 - 迭代算法：$x_{k+1}=f(x_k)$，给定一个任意$x_0$，一直迭代，$k\rightarrow \infty$时，$x_k \rightarrow x^*$，其实就是这样一个数列的极限 

#### ②f(v)的压缩性质 - 进而求解$v^*$与$\pi ^*$

- 给出不加证明的结论，f(v)可证得具有压缩映射的性质，即$$||f(v_1) - f(v_2)|| <= \gamma ||v_1 - v_2||$$
- 求解$v^*$：因此贝尔曼最优公式满足压缩映射性质，我们可以有：此公式一定有解，且解唯一，可以使用迭代算法去求解，因此我们上面说，给一个初始$v$值去迭代，迭代到最后$v$会收敛到$v^*$，即最优状态值
- 求解$\pi *$：得到$v^*$后，带入到贝尔曼方程可以求出$\pi^*$，即最优策略，**这个最优策略，实际上就是，对于当前状态s，选择动作值最大的动作的概率为1，选择其他动作的概率为0，每个状态都这样做，最终就形成了最优策略**

### 4.最优策略的有趣性质

- 影响最优策略的因素：看贝尔曼方程即可得知，v和π是未知量，$r , \gamma,p(r|s,a)$是已知量，故后面三者是影响最优策略的因素，而关于$p(r|s,a)$，其为系统模型，故一般来说$r,\gamma$是影响最优策略的因素
  - 改变$r,\gamma$都可能会使策略发生变化
- 关于奖励的绝对值与相对值：真正影响策略的是奖励的相对值，也就是说对所有奖励进行同比例放缩，最优策略是保持不变的
- 在第一章我们也说过，“奖励”实际上是一个人机交互的工具，因此我们可以通过调整奖励来得到想要的策略
- 避免无意义的绕路：最优策略会避免无意义的绕路

##  四、值迭代与策略迭代

- 引言：从这章开始学习强化学习算法，本章介绍三种方法：值迭代、策略迭代以及截断策略迭代，其中值迭代和策略迭代都是截断策略迭代的特殊情况，因此截断策略迭代更具一般化
- 本章介绍的三种方法都是基于模型的，即我们需要实现知道系统模型，**拥有对环境的完整知识，知道环境的状态转移概率和奖励函数**

### 1.值迭代算法

- 值迭代的方法基于贝尔曼最优方程

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/eef572175d6c2109d731cdd72c2b36f2.png" alt="image-20250611162704815" style="zoom:67%;" />	

- 每次迭代的两个步骤
  1. 策略更新：$\pi_{k+1}={argmax}_\pi(r_\pi + \gamma P_\pi v_k)$，其中$v_k$是上一次迭代得到的值
  2. 值更新：$v_{k+1}=r\pi_{k+1}+\gamma P_{\pi_{k+1}}v_k$,策略是上面迭代得到的策略，求出$v_{k+1}$后继续用于下一次迭代
- 对于每次迭代的值$v_k$：它不是任何一个状态的状态值，尽管时其会收敛到最优状态值，但k有限时，$v_k$可能不满足任何一个贝尔曼方程，因此也不是某一个策略的状态值
- 伪代码

```cpp
初始化：已知模型：对任意(s,a)对应的p(r|s,a)和p(s'|s,a)，设定初始值v0
目标：求解最优状态值和最优策略

当v_k未收敛时，进行如下迭代：
	(遍历)对于每个状态s:
		(遍历)对于每个动作a
			计算动作值q;
		选择最大动作值的action;	
		策略更新：即π_{k+1}(a|s)=1; //迭代第一步
		值更新：v_{k+1}(s)=max q_k(s,a) //迭代第二步
```

### 2.策略迭代算法

- 第k次迭代的两个步骤
  1. **策略评价**（policy-evaluation)：评估上一次得到的策略$\pi _k$，评估方法就是求在当前状态下使用策略$\pi _k$得到的贝尔曼方程$v_{\pi_k}=r_{\pi_k}+\gamma P_{\pi_k}v_{\pi_k}$，从而得到$\pi _k$的状态值
  2. **策略改进**（policy improvement）：用来改进策略以得到更好的策略，方法是在第一步得到$v_{\pi_k}$后，利用$\pi_{k+1}=argmax_{\pi}(r_\pi + \gamma P_\pi v_{\pi_k})$
- 值迭代和策略迭代的区别
  - 值迭代：对于每个(s,a)，初始化一个v，每次选择q值最大的策略，使用这个策略去更新v，下一轮再拿更新后的v去找最优策略，即**反复使用贝尔曼最优方程去更新v**，每一步都隐含策略更新；
  - 策略迭代：对每个(s,a)，初始化一个策略π，先求在这个π下的状态值v来评估这个策略，再拿这个v来求最优策略（q值最大），（显示的交替进行PE和PI）
- 三个关键问题（这里看gpt学了，以后有需要再看书）
  1. 策略评价过程中，如何计算状态值$v_{\pi _k}$
     - 已知$\pi_k$，$p(r|s,a),p(s'|s,a)$，使用贝尔曼方程求解即可
     - 如何解贝尔曼方程：参考书上第二章求贝尔曼方程的方法，法一直接解矩阵方程，这个涉及到求逆矩阵，一般不使用；法二是**迭代法近似求解**，即初始化一个$v_0$，一步步迭代出最终的$v_{\pi_k}$
  2. 策略改进中，为什么新策略$\pi _{k+1}$比之前的$\pi _k$更好
     - 首先复习策略更好的定义：指的是对于**每一个状态**，该策略的状态值**都比**其余策略的状态值要大
     - 策略改进定理：如果$\pi_{k+1}=argmax_{\pi}(r_\pi + \gamma P_\pi v_\pi)$，则$v_{\pi_{k+1}}>=v_{\pi_k}$，人话：即选择当前状态值下动作值最大的action对应的策略，一定比之前的策略好
  3. 为什么这个算法可以最终收敛到最优策略
     - 这个直接类比**数列单调有界定理**
     - 首先策略是**有界的**，即对于有限个状态，有限个动作而言，策略的总数的有限的
     - 其次由第二点得知，每轮迭代都会使策略更好，可以理解为策略性能是**单调递增的**
     - 由单调有界定理得知，策略一定会收敛到一个最优策略

### 3.截断策略算法

- 值迭代 VS 策略迭代
  - 策略迭代：从一个策略$\pi _0$出发，求状态值$v$，再拿求到的$v$，去找最优策略（动作值最大）
  - 值迭代：从值$v_0$出发，先求在这个$v$的最优策略$\pi$，再拿$\pi$去更新$v$

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/35b9fcd1331500360b4ac7d0a544941d.png" alt="image-20250623100036683" style="zoom: 80%;" />	

![image-20250623100754935](https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/d03c806c210d34904716b92e2ec371b0.png)

- 由上图可以看到，如果将值迭代的Value初始值设为$v_{\pi _0}$（策略迭代由初始策略求出的状态值），那么两种方法求最优策略，以及求Value的方式都非常相似，求Policy就是算出动作值最大的Action，求Value就是贝尔曼方程
- 但实际在求Value的过程中，二者是有区别的
  - 值迭代的value，就是求一次贝尔曼方程，拿上一次的$v_k$来求本次的$v_{k+1}$
  - 策略迭代的Value，也是求贝尔曼方程，但是用迭代法去求（需要迭代多轮贝尔曼方程，最终收敛）

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/0e39d742100f06ea1202ff9f5cdb2015.png" alt="image-20250623101749962" style="zoom:67%;" />	

- 所以就会想有没有这样一个中间状态，在迭代法求贝尔曼方程时，规定只迭代到第j步，得到的$v$再用于下一步的迭代，这样的方法就是**截断策略迭代**，显然截断策略迭代是值迭代和策略迭代的一种一般形式，当$j = 1$时就是值迭代，当$j\rightarrow \infty$时就是策略迭代
- 在实际使用中，我们使用的策略迭代实际上都是截断策略迭代，因为不可能去计算无穷步，只能说到达某一步后，看前后两个$v$差别，如果差别很小则认定收敛，停止迭代

## 五、蒙特卡罗方法

> 第四章的值迭代和策略迭代都是基于模型的方法（model-based），本节将第一次介绍无模型的方法（model-free） - 基于蒙特卡洛的方法

### 1.一个例子🌰

- 问题描述：投掷硬币，设结果为X，若硬币为正面，则X = +1，若硬币为反面，则X = -1，求解多次投掷后X的期望（平均值）
- 方法一：基于模型的方法，我们**事先知道**投掷为正面的概率为0.5，投掷为反面的概率为0.5，可以简单算出期望为0
  - 问题：很多问题中，我们可能事先不知道概率分布，因此，model-free的方法出现了
- 方法二：model-free方法，思路也很简单，不知道概率分布，就先采样n次，求平均，算出概率分布；由**大数定律**可知，当n越大时，得到的概率分布就越准确

### 2.MC Basic - 最简单的基于蒙特卡罗的强化学习算法

- 基于第四章的策略迭代算法，回顾策略迭代，其步骤分为”策略评估“和”策略改进“两部分，其中策略评估
- 而在求动作值时，我们之前的方法，是基于$q_{\pi_k}(s, a) = \sum_{r} p(r \mid s, a) r + \gamma \sum_{s'} p(s' \mid s, a) v_{\pi_k}(s')$，即当下即时奖励 + $\gamma$*未来奖励，而这种方法下，$p(r|s,a)$，即在当前状态下采取动作a得到的奖励r，是模型信息，我们现在要做model-free，自然不能使用这种方式计算
- 回到动作值最初的定义，即$q_{\pi_k}(s, a) = \mathbb{E}[G_t \mid S_t = s, A_t = a]$

> 定义：动作值定义为**一个状态采取一个动作之后**，然后按照策略$\pi$行动，获得的回报的期望值

- 正式讲讲算法

  - 核心思想：用**实战经验（完整轨迹）**来估计动作值，而不是用贝尔曼方程和环境模型

  - 前提假设：可以和环境交互，获得完整的**episode（s,a,r)**；不知道$p(r|s,a)$，不知道奖励函数，只能靠试出来
  - 求动作值q的思路，$q_{\pi_k}(s, a) = \mathbb{E}[G_t \mid S_t = s, A_t = a]$
    1. 从任意的$(s,a)$的组合出发，初始化任意策略$\pi$，得到一个episode$(s_1,a_1,r_1,s_2,a_2,r_2)$，将该回合得到的Return记为$g(s,a)$，即$g(s,a)$是公式中$G_t$的采样
    2. 按照上面的步骤采样很多次，得到很多个$g(s,a)$，求平均值，得到的就是$G_t$
  - 一句话总结：没有模型的时候，依赖于数据
  - 算法步骤

  ```cpp
  for i : s:
  	for j : a:
  		1.做采样
  		2.计算回报G（得到动作值q，进而得到状态值v）
  		3.策略评估
      4.策略改进
  ```

- 在实际使用中因性能原因并无MC Basic算法，该算法只是将model-based转换为model-free一个最基本的思想

### 3.MC Exploring Starts算法 - 基于MC-Basic的优化

- 相对于效率极低的MC-Basic有两个优化

#### 1)第一个优化 - 让数据的使用更加高效

- MC-Basic的缺陷：假设有两个episode，可见第一个路径包含了第二个路径，所以对于第二个路径计算折扣回报G就是重复的

  - $s1(a2)\rightarrow s2(a4)\rightarrow s3(a5)\rightarrow...$
  - $s2(a4)\rightarrow s3(a5)\rightarrow...$

- **first-visit method（MC-Basic的 缺陷方法）**

  - 将一个episode中的访问一个s1(a2)这样一个state-action对，称为一个visit
  - 对于一个episode，以$s1(a2)\rightarrow s2(a4)\rightarrow s3(a5)\rightarrow...$为例，只考虑$s1(a2)$，用剩下所得到的Return来估计$s1(a2)$的动作值

  > 首先，这些方法的目的，都是求动作值，具体说就是求一个(s,a)下的动作值，然后在model-free中，求动作值是按照动作值的定义去求的，也就是需求去求G的期望，所以这些方法就是求一个(s,a)下G的期望对吗 对于Initial-visit方法，就是按照求折扣回报来求G的期望的，特别之处就在于，对于一个(s,a)，只有在第一次遇到它的时候，才计算它的折扣回报，而在之后遇到时，就不再去求它的折扣回报了，直接照搬之前求的$G_t$  

  - 其不足之处：对于每个(s,a)只有在第一次遇到的时候计算一次$G_t$，后续再遇到也不再计算了，这就浪费了后面的经验，而且因为每个(s,a)只计算了一次$G_t$，这样Q值的收敛速度就会很慢，且首次访问(s,a)时所求的$G_t$可能偏大或偏小，存在后续无法纠正的偏差；

- **every-visit（优化方法）**

  - 对于任意(s,a)，每次访问到它的时候，都计算一次$G_t$，作为一个样本，结束后求所有样本的期望作为这个(s,a)的$G_t$，依次求出(s,a)的Q值，对于每一个(s,a)，都可以看做以这个(s,a)为开始的一个episode，

    > 引自知乎：表面上every-visit 方法每次都重复计算, 看起来好像没有减少重复计算量, 但是由于无论起点在哪, 每遇到一次就增加一次样本量, 所以重复实验的次数N确定的情况下, 这种方法获取的样本量远超MC basic算法, 那么我们就能适当减小N的值, 也相当于变相减少了计算量

  - 这样的好处不言而喻：对于每个(s,a)都可以充分利用经验，且Q值收敛速度较快，因为求的是所有样本的期望，所以大大降低了所求$G_t$偏大or偏小的可能


#### 2)第二个优化 - 让策略的更新更加高效

- 缺陷方法（MC-Basic的方法）
  - 对于每个(s,a)，收集以(s,a)为start的episode，求出每个对应的样本G，最终求平均求出最终的$G_t$，最终求出Q值，再更新策略
  - 这样的问题就是，对于每个(s,a)，都要等到所有episode全收集完后才能计算，就需要等，效率自然就低；
- 优化方法：improve the policy episode-by-episode.
  - 就类似于迭代法求贝尔曼方程
  - 得到一个episode，就立即估计Q值，拿到Q值后就直接开始改进策略；即得到一个episode就改进一次策略，得到一个episode就改进一次策略
  - 这样就不用等，效率upup！
- 在实际计算的时候，拿到一个episode时，我们通常是从后往前计算，比如$s1(a2)\rightarrow s2(a4)\rightarrow s3(a5)$我们可以先计算(s3,a5)的回报这样从后往前计算，这样计算前面的就可以用到后面的值，联想到贝尔曼方程的递归形式就可以理解了。

​		<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/e78991732d63d0f417f98dd002031aec.png" alt="image-20250712110300498" style="zoom:67%;" />	

#### 3)Exploring与Starts的必要性

- exploring探索：指的是对于每一个(s,a)，都需要有以此(s,a)为起始的episode，必须要确保每一个对应的episode都被visit到；如果漏掉一个(s,a)没有计算，恰好这个(s,a)的episode对应最优的Q值，就会被错过

## 六、随机近似理论(Stochastic Approximation - SA)

> SA：在不确定性（噪声）下，逐步逼近某个真实值或最优解的方法

### 1.RM算法

- 回顾期望值估计

  - 上一章介绍过：将所有采样求平均，根据大数定律，样本数越多估计的值越准，这种方法称为非增量式的方法，对样本数的要求较高，且需要将全部样本都收集完成后才能去求平均

  - 此外还有一种**非增量式的方法**，（迭代式），来几个就先计算几个，每次计算不需要求所有样本平均，而是使用前一个的结果迭代

    ![image-20250701135505330](https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/65c6d4e111a4d005ba966f8dcf198b2d.png)	

- RM算法概述

  - 下面的随机梯度下降（SGD），实际上是RM算法的一种特殊情况
  - RM要求解的是一个方程，一个自变量为w的方程g(w) = 0
    - 很多问题都可以转换为这样一种方程，因此都可以套用RM算法

- 随机近似（SA），是逼近真实值或求解最优解的方法，而解决这类问题的方法有很多，梯度下降也是其中一种，SA相对于其他方法的优势在于不需要知道方程or目标函数的表达式，只知道输入和输出，就能去解决问题（求方程，但是连这个方程什么样都不知道）

- ==类似神经网络，就是一个黑盒，只看到输入输出，求解g(w) = 0的问题，就转换为在神经网络中，输入一个什么，可以得到输出0==

- 算法内容

  - 目标：求解$g(w)  =0$，假设最优解为$w^*$

    <img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/6a5fa9ffc2d54df39423b5caa23bc35e.png" alt="image-20250701145629008" style="zoom:67%;" />	

  - $w_{k+1}$：第k + 1轮的期望值

  - $g(w_k,\eta _k)$：是一个函数，$g(w_k,\eta _k) = g(w_k) + \eta _k$，即输出 + 噪声，理解为输出g一个带有噪音的观测

  - $\alpha _k$：是一个正数

  - 已有信息：输入信息$w_k$，以及$g(w_k,\eta _k)$，输入一个$w$，得到一个$\tilde{g}$

  - 还是那个思想：没有模型，就用数据

  - RM为什么能找到解（**RM的收敛性**）：直接先记结论吧，RM是收敛的


### 2.随机梯度下降

- SGD要解决的问题：$J(\theta) = E[f(\theta,X)]$，其中w是需要优化的参数，X是随机变量，SGD要解决如何优化参数$\theta$，使目标函数的值达到最小（目标函数可以是损失函数，这样就可以求解最小loss），求最小值就是用梯度下降，求最大值就是用梯度上升
- 求解上面问题的方法有很多，这里介绍三种，分别为梯度下降，批量梯度下降和随机梯度下降，这里只拎SGD出来说说
- 前提：在强化学习中，特别是**策略梯度**方法中，我们需要**优化某个函数**来**改进策略或值函数**，SGD就是最常见的一种优化方法
- SGD基本形式：$\theta \leftarrow \theta - \alpha \cdot \nabla_\theta \hat{J}(\theta)$
  - $\theta$：模型参数
  - $\alpha$：学习率
  - $\nabla_\theta \hat{J}(\theta)$：对参数$\theta$的导数，作用到目标函数$J(\theta)$上

<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/32095df914980937db3119d78a9c8f97.png" alt="image" style="zoom: 67%;" />	

## 七、时序差分方法

- 第五章的MC是介绍的第一个model-free的方法，本章的TD是我们要介绍的第二个model-free的方法

- MC的方法是一个**非递增的方法**（non-incremental），TD是一种**递增**（迭代式）的方法（incremental）的方法

  - ==非递增：一次性更新，必须等待**整个episode**才能更新，这里要和every-visit区别开，every-visit是针对每个(s,a)开始的episode，都求一次Q值，和这里等待整个episode更新不冲突
  - 递增：每收到一条新数据，就可以立即更新，无需等待完整episode
  - ==上面两点是TD和MC的核心区别==

  > **例子：玩游戏估分数**
  >
  > 假设你是裁判，观察选手玩一个游戏：
  >
  > - **MC 方法**：你会等选手完整打完一局，再根据最后得分来给出评估；
  > - **TD 方法**：你在选手每打一小段，就根据当前表现 + 预测他接下来表现，对当前表现打分。
  >
  > 所以：
  >
  > - **MC** 相当于“事后总结”
  > - **TD** 相当于“边看边预测”

### 一、状态值估计：最基础的时序差分

- 算法目的：求解在一个给定策略$\pi$下，每个状态的状态值
- 为什么要求解状态值：结合之前的策略迭代算法，求解状态值相当于做了策略评估，服务于后续的状态改进寻找最优策略
- TD算法既可指大类TD算法，本章所有算法都可称为TD算法，也可指特定算法，即本节所讲的求状态值的最基础的算法
- 初始数据：给定一个策略$\pi$的经验样本$(s_0,r_1,s_1,r_2,...,s_t,r_{t+1})$，其中$t=0,1,2...$为采样时刻
- 算法内容

$$
v_{t + 1}(s_t) = v_t(s_t) - \alpha_t(s_t)[v_t(s_t) - (r_{t+1} + \gamma v_t(s_{t+1}))],\\
v_{t+1}(s) = v_t(s),当s \not = s_t
$$

- 算法说明
  - $v_t(s_t)$是在t时刻对$v_\pi(s_t)$的估计
  - $\alpha$是学习率
  - 先解释第二行：除了当前时刻访问的状态$s_t$外，其余状态的状态值不变。挺好理解的，就是只更新当前访问状态的状态值，没访问的状态不要动

- 再解释下主要的第一行
  - 实际就是**新的状态值估计 = 当前估计的状态值 - 学习率 * TD误差**，换句话说就是用==学习率 * TD误差来调整当前的估值==
  - TD target：即$r_{t+1} + \gamma v_t(s_{t+1})$，为当前经验下的目标估计值
  - TD error = $v_t(s_t) - (r_{t+1} + \gamma v_t(s_{t+1}))$，为估计和目标值之间的误差

​			<img src="https://raw.githubusercontent.com/mumushu1/Pictures/main/09bec81d6a60f77a4a968d2c6e6a7a0f.png" alt="image-20250713085953823" style="zoom:67%;" />	

- 算法概括：**不断修正对状态值的估计，用实际经验中的“短期反馈 + 下一步预测”为目标，去逼近真正的状态值**
  - 我们用当前一步的经验，对“真实状态价值”的猜测（目标）即为**TD target**
  - 当前估计和 TD target 之间的偏差，即为**TD error**
- 下标$t$的意义：agent与环境交互的第$t$步，是环境的时间步，也是算法“学习/更新”的次数
- 收敛性

> **TD** 在表格法（tabular）、固定策略和合理学习率下是**理论上严格收敛的**。
> 但在使用神经网络等非线性逼近器时，可能不收敛，需要额外处理。

- 算法特点
  - 只能估计给定策略的状态值
  - 不能估计动作值，不能计算最优策略

### 二、动作值估计：Sarsa

- Sarsa是什么：State - action - reward - state - action

- Sarsa类算法做的事：给定一个策略，可以估计Action Value，因此可以做策略评估，再结合策略改进，找到最优策略
- 初始数据（没有模型，需要数据）：给定一个策略$\pi$的经验样本$(s_t,a_t,r_{t+1},s_{t+1},a_{t+1},r_{t+2}...)$
- 算法内容

$$
\begin{align}
q_{t+1}(s_t, a_t) &= q_t(s_t, a_t) - \alpha_t(s_t, a_t) \left[ q_t(s_t, a_t) - \left( r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1}) \right) \right], \tag{1} \\
q_{t+1}(s, a) &= q_t(s, a), \quad \forall (s, a) \ne (s_t, a_t). \tag{2}
\end{align}
$$

- 对比上面的TD算法，区别就是将$v_{t+1}(s_t)$改为了$q_{t+1}(s_t,a_t)$，所以可以概括为，**不断修正对当前(s,a)下Q值的估计，用实际经验中的短期反馈+下一步预测为目标，去逼近真正的Q值**

- 算法特点

  - 可以求每个(s,a)对应的Q值，因此对于每个(s,a)，我们可以得到最优动作，这是**局部最优**
  - ==局部最优不一定是全局最优==，最优路径也可能出现在没有被探索过的格子上

  > 想象你在一个山谷里爬山，目标是找到**最高的山峰**（全局最优）。
  >
  > - 你爬到某个小山峰，看四周发现都比它低，于是你认为这就是最优解。
  > - 但其实远处还有一个更高的山峰，只是你没去过那里。
  >
  > 这个你找到的小山峰就是“**局部最优**”，而真正最高的那个山峰才是“**全局最优**”。

  <img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/4be723e6a7ea6794045f7ebf934126c9.png" alt="image-20250713143928976" style="zoom:50%;" />	

### 三、动作值估计：n-step Sarsa

- **Sarsa的一种结合蒙特卡罗方法的变形**
- 算法内容

$$
\begin{align}
q_{t+1}(s_t, a_t) &= q_t(s_t, a_t) - \alpha_t(s_t, a_t) \left[ q_t(s_t, a_t) - G_t^{(n)} \right], \tag{1'} \\
q_{t+1}(s, a) &= q_t(s, a), \quad \forall (s, a) \ne (s_t, a_t), \tag{2'}
\end{align}
$$

- 对比普通Sarsa，围绕计算Return的方式变形
  - 普通Sara，或1-step Sarsa：只使用当前这一步的经验（一个即时奖励r + 下一个Q值）来更新
  - **n-step-Sarsa：使用了接下来n步的实际奖励，再加上第n步的Q值，作为目标更新**
  - 蒙特卡罗方法计算Return：使用完整episode的经验来更新
- n->1时，就接近普通Sarsa，n->无穷时，就接近蒙特卡罗

<img src="https://raw.githubusercontent.com/sleepyDev0x/Pictures/main/70b00165b7d238fa8f21f120743eb4b0.png" alt="image-20250713145715987" style="zoom: 50%;" />	

### 四、Q-learning

#### 1)算法概述

- Q-learning与上面的TD算法最大的区别：**直接估计最优Q值，所以不需要在策略评估和策略改进两个步骤间来回运行**
- 算法内容

$$
\begin{align}
q_{t+1}(s_t, a_t) &= q_t(s_t, a_t) - \alpha_t(s_t, a_t) \left[q_t(s_t, a_t) - \left( r_{t+1} + \gamma \max_a q_t(s_{t+1}, a) \right)\right], \tag{1} \\
q_{t+1}(s, a) &= q_t(s, a), \quad \forall (s, a) \ne (s_t, a_t). \tag{2}
\end{align}
$$

- 可以看到**和Sarsa的区别就在TD target部分**，如果还是拿贝尔曼方程去类比，SarsaTD target是普通贝尔曼方程，而Q-learning的TD target部分是贝尔曼最优方程，最终得到的Q值是一个最优Q值

#### 2)On-policy与Off-policy

- On-policy：用某个策略采样行为，并用它的**结果更新这个策略本身**
- Off-policy：用一个策略来采样行为（探索用），但**学习的是另一个策略的值**

> 🧠你是个老师（算法），在指导一个学生（agent）做题：
>
> - **On-policy**：你让学生用你教的方法做题，然后根据他做题的情况，**改进你教的方法**。（Sarsa）
> - **Off-policy**：你让学生自由尝试各种解法（甚至乱来），但你只根据他的结果去**改进你心中理想的标准解法**。（Q-learning）

- **再看Q-learning：即使实际中这一步选错了，我更新的时候仍然[假设你选的是最优的那步]来估算Q值**

  - 只要探索的够多，且学习率合理递减，最终是会收敛到最优策略的

  > Q-learning 的关键在于：**探索可以犯错，但更新时总是对未来充满信心（最大 Q 值）**。
  >  这种 off-policy 学习方式，只要探索足够，最终能收敛到最优策略。
