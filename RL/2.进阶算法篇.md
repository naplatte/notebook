## 从策略梯度到PPO

> 策略梯度基础见<1.强化学习数学原理>

### 一、策略梯度的问题

- 在REINFORCE中，更新是$\theta \leftarrow \theta + \alpha \nabla_{\theta} J(\theta)$，即朝着期望回报的梯度方向走一步

- 问题：学习率过大：可能策略跳远，性能崩掉；学习率过小，则收敛太慢

### 二、TRPO的出现

- **TRPO：Trust Region Policy Optimization，信任区域策略优化**

- 核心思想：每次更新时，不仅要让目标变好，还要保证**新策略与旧策略不能差太多**

- 如何做到

  - 不直接使用梯度更新策略，而是通过**信任区域**约束策略更新的范围 - 即新策略与原策略的差异不能超过一个 “可信” 的区域
  - 这个新旧策略的差别，用**KL散度**来量化
    - KL散度就是两个策略概率分布的差异度
    - KL = 0，则两个策略一摸一样，KL越大，则二者差异越大

- 优化目标：$L(\theta)=\max_{\theta} \mathbb{E}_t \left[ \frac{\pi_{\theta}(a_t \vert s_t)}{\pi_{\theta_{\text{old}}}(a_t \vert s_t)} \hat{A}_t \right]$，其中$\mathbb{E}_t \big[ D_{KL}(\pi_{\theta_{\text{old}}} \parallel \pi_{\theta}) \big] \leq \delta$，即KL散度约束在一个阈值中

  - $E_t[·]$为对“旧策略”下采样到的数据做期望（也就是用过去采的轨迹）
  - $r_t(\theta) = \frac{\pi_{\theta}(a_t \vert s_t)}{\pi_{\theta_{\text{old}}}(a_t \vert s_t)}$，比率项，即在$state = s_t$下，新策略选择动作a的概率与旧策略选择动作a概率的比值，表示**在新策略下，这个动作比在旧策略下更“受欢迎”多少倍**，若比率项>1，则新策略更倾向于这个动作
  - 优势函数$\hat{A_t}$：表示这个动作在当时是否比平均水平更好
    - 优势函数定义为：$A^{\pi}(s_t, a_t) = Q^{\pi}(s_t, a_t) - V^{\pi}(s_t)$，即**动作值 - 状态值，表示这个动作比平均水平好多少**，**这里实际上就是Critic干的事情（策略评估）**
    - 写为$\hat{A_t}$是因为其真实值是未知的（无法精确知道未来所有回报），我们只能用采样或近似来估计（MC，TD等方法）
    - $\hat{A_t}> 0$：动作比平均水平更好 → 希望 **增加** 它的概率（让$r_t$大一些）
    - $\hat{A_t}<0$：动作比平均水平更差 → 希望 **减少** 它的概率（让$r_t$小一些）

  - 理解$r_t(\theta)·\hat{A_t}$：前者表示新策略对这个动作改了多少概率，后者表示这个动作好不好

  > 1. 如果 $\boldsymbol{\hat{A}_t > 0}$（动作好），而且比率 $\boldsymbol{r_t(\theta) = \frac{\pi_\theta(a_t \vert s_t)}{\pi_{\theta_{\text{old}}}(a_t \vert s_t)} > 1}$（新策略增加了概率）：→ 积极的加成，目标函数变大 ✔️。（说明新策略做对了，给好动作更多概率。）   
  > 2. 如果 $\boldsymbol{\hat{A}_t > 0}$（动作好），但比率 $\boldsymbol{r_t(\theta) = \frac{\pi_\theta(a_t \vert s_t)}{\pi_{\theta_{\text{old}}}(a_t \vert s_t)} < 1}$（新策略反而减少了概率）：→ 对目标函数有负面影响 ❌。（说明新策略做错了，应该惩罚它。）   
  > 3. 如果 $\boldsymbol{\hat{A}_t < 0}$（动作差），而比率 $\boldsymbol{r_t(\theta) = \frac{\pi_\theta(a_t \vert s_t)}{\pi_{\theta_{\text{old}}}(a_t \vert s_t)} < 1}$（新策略减少了概率）：→ 积极效果 ✔️。（说明新策略正确降低了坏动作的概率。）   
  > 4. 如果 $\boldsymbol{\hat{A}_t < 0}$（动作差），但比率 $\boldsymbol{r_t(\theta) = \frac{\pi_\theta(a_t \vert s_t)}{\pi_{\theta_{\text{old}}}(a_t \vert s_t)} > 1}$（新策略反而增加了概率）：→ 负面效果 ❌。（说明：已按 Markdown 语法，用 `$` 包裹公式，可直接在支持 LaTeX 渲染的 Markdown 编辑器中显示 ）
  >
  > **我们希望新策略增加好动作的概率，减少坏动作的概率。目标函数就是在奖励这种行为（增加好动作的概率，减少坏动作的概率）、惩罚反向的行为。**

  - ==旧策略下，每个时间步t都有一个$(s_t,a_t)$，每个$(s_t,a_t)$都可以求出$r_t(\theta)·\hat{A_t}$，对所有时间步下$(s_t,a_t)$的$r_t(\theta)·\hat{A_t}$求期望（平均），表示**在整个旧策略分布下，新策略的平均改进效果**==

- 具体步骤（初始时是一个随机策略$\theta$）

  1. 采样数据：在旧策略$\pi_{\theta_{old}}$中采样多条trajectory，对每个时间步计算$\hat{A_t}$
  2. 构建目标函数：对每条trajectory的所有时间步计算$r_t(\theta)·\hat{A_t}$，再对所有数据(所有trajectory的所有时间步)求期望（平均），得到$L(\theta)$，同时计算KL散度
  3. 梯度计算：对$L(\theta)$求$\theta$的梯度，TRPO使用的是**二阶近似** （KL 约束的二阶泰勒展开）+ 共轭梯度法，这部分先放一下
  4. 线搜索：沿梯度方向尝试最大的α，找到最大步长对应的$\theta$，使：①目标函数$L(\theta)$增大；②KL散度不超过阈值
  5. 更新策略：得到新策略$\theta$，下一轮迭代使用这个新策略继续采样

### 三、PPO

- TRPO的缺点

  - 约束是一个“二次规划”问题，二阶约束，实现复杂，计算量大
  - 在工业界（比如 OpenAI）应用时发现，TRPO 太“重”了，不方便大规模跑实验

- **PPO：Proximal Policy Optimization，近端策略优化算法**

- 出现的可能性：能否用一个更简单的方法来近似TRPO的效果 -  **Clipped Surrogate Objective**（裁剪目标函数）

  

$$
L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right]
$$

- 拆解公式，我们可以将其看为$L_t^{CLIP}(\theta) = \min(\text{term}_1, \text{term}_2)$

  - $ r_t(\theta) \hat{A}_t$，同TRPO

  - $\text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) $，裁剪比率在[1−ϵ,1+ϵ]，防止$r_t(\theta)$变得太大或太小，限制单步策略更新幅度

    - 若动作是好动作（$\hat{A_t}>0$），我们当然希望新策略**增加这个动作的概率**，即我们希望$r_t(\theta)$是增大的

      - 如果$r_t(\theta) < 1 + \epsilon$，表示在信赖域内(term₁ < term₂)，我们可以直接使用$r_t(\theta) \hat{A}_t$，即选择term~1~
      - 如果$r_t(\theta) > 1 + \epsilon$，超出信赖域，我们选择term~2~，即$(1+\epsilon)\hat{A_t}$
      - 直观理解：对于好的动作，我们鼓励其概率增加，但绝不允许它增加得太多太快

    - 反之，对于坏动作

      - 如果在信赖域内，直接term~1~即可
      - 如果超出信赖域，我们选择$(1 - \epsilon)\hat{A_t}$

      - 直观理解：对于不好的动作，我们鼓励其概率减少，但也不能减的太多太快

- 没有KL约束，用裁剪函数直接限制策略更新，可以直接使用**普通梯度上升/Adam/SGD来更新$\theta$**

## DDPG

### 一、Actor-Critic回顾

- 值函数方法与策略梯度方法
  - 值函数方法学习一个值，通过值影响策略
  - 策略梯度方法直接学习策略
- 二者优缺点
  - 值函数方法：适合离散动作空间，但难以直接用于连续动作空间（每个state对应无限个action，无法量化Q(s,a)）
  - 策略梯度方法：适合连续动作空间，可学习随机策略；但方差高，收敛慢
- 二者的结合 - 即Actor-Critic
  - Actor（策略网络），使用策略$\pi(a|s)$来选动作
  - Critic使用动作价值函数$Q(s,a)$来估计好坏
