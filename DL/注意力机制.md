## 零、预训练 & 词向量

### 0.1 预训练

- **预训练 = 生成词向量 + 迁移到下游任务**
- 预训练的过程，就是在 **大量文本** 上学习每个单词的 **最佳词向量**，让其捕捉语义信息
- 训练完成后，模型可以将新数据中的单词转换成合理的词向量，以便用于下游任务
- Word2Vec 得到的是 **静态词向量**，不能区分不同语境下的一词多义
- BERT，GPT 得到的是 **动态词向量**，能根据上下文调整词的意义
- 预训练完的语言模型，本质上就是一个 **能为任何文本生成高质量词向量的系统**，这就是 NLP 预训练的核心价值

### 0.2 静态 & 动态 词向量

- 静态词向量：预训练阶段，模型计算出 "apple" 的固定词向量，比如 (0,1,12)，之后进行下游任务中，"apple" 的词向量就会被赋予（0，1，12）

- 动态词向量

  - 预训练时，模型 **学习的是如何根据上下文生成词向量**，而不是给每个单词分配一个固定的向量
  - 词向量是根据上下文动态变化的，**即同一个单词在不同语境下词向量不同！**

  ```py
  Sentence 1: "I eat an apple."      → apple 的词向量是 (0.2, 1.1, 11.5)
  Sentence 2: "Apple released iOS."   → apple 的词向量是 (5.6, 0.9, 2.1)
  ```

  - 因为 **BERT 不是单独看 "apple" 这个词，而是结合整个句子计算它的词向量**。
  - 所以，**在新的任务中，BERT 仍然会根据上下文重新计算词向量**，**而不是简单复用原来的词向量。**

### 0.3 NLP 相关问题

- nlp 是什么：研究人与计算机之间，使用自然语言进行通信的理论和方法
- nlp 相关领域
  - 文本分类：情感分类、主题分类
  - **生成式任务**：根据一段文本，生成另一段文本
    - 翻译：汉译英
    - 文本摘要：提取文章主题
    - **对话** 系统：gpt
    - **问答** 系统：gpt
  - 语音识别

## 一、Attention

### 1.1 人类的注意力机制

- 从众多信息中选择出对处理当前任务目标更为关键的信息

### 1.2 本质思想

- 思想类似人类的注意力机制，即从大量信息中 **有选择地筛选出少量重要信息** 并 **聚焦到这些重要信息上**，**忽略不重要的信息**。

- attention 的其他作用
  - LSTM 克服了 RNN 长距离缺乏依赖的问题，但 **单词超过 200** 时就会失效
  - attention 进一步解决了长距离依赖（超长距离），且具有 **并行计算能力**
  - 弹幕数据集其实这点影响不大，因为大多都是短文本
- Q、K、V
  - 核心思想：**动态计算不同位置之间的相关性，赋予输入序列中不同位置不同的权重，从而捕捉长距离的依赖关系**
  - 直观理解
    - Query：表示查询向量，告诉我们要关注什么
    - Key：表示键向量，帮助 Query 找到匹配的信息
    - Value：表示值向量，存储最终输出的信息
  - 类比搜索引擎
    - Query：搜索的关键词（eg：搜索什么是 attention）
    - Key：网页的关键词标签（eg：”深度学习“，”注意力机制 ")
    - Value：网页的实际内容
  - Q，K，V 的计算到底干了一件什么事：通过 **Query 和 Key 计算注意力权重，然后用权重去加权**

<img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/attention-%E8%AE%A1%E7%AE%97%E5%9B%BE.png" alt="img" style="zoom: 80%;" />

- 计算过程
  - 首先做 Q 和 K 的点乘，得到结果 S~i~
    - 点积可以衡量两个向量的 **相似程度**
    - **若点积值较大，则 Q 和 K 的方向接近，应该分配更大的注意力权重**
    - **若点积值较小，则 Q 和 K 的方向差异较大，应该分配更小的注意力权重**
  - 再将 S~i~进行 Softmax 处理，得到概率 a~i~
    - 为什么 Softmax 要除以一个 $/sqrt k$：稳定梯度，防止梯度消失和梯度爆炸
  
  $$
  \alpha_i = softmax(\frac{f(Q, K_i)}{\sqrt{d}_k})
  $$
  
  
  
  - 最后针对算出的权重 a~i~，对 V 中所有 Values 加权求和，得到 attention 向量（Attention Value = a~1~V~1~ + a~2~V~2~  + ... + a~m~V~m~）
  
- 关于最后的结果 Attention 向量的理解

  - **Attention 向量是最终的加权表示，综合了输入序列的所有重要信息**
  - 它是 **上下文感知的表示**，可以动态关注不同部分，而不是固定地使用某个局部信息
  - **让模型知道，上下文哪里更重要，哪里更不重要**

### 1.3 Self Attention 模型

整体架构：即 QK 点乘，Softmax，对 V 加权

- Attention 是一种思想，**Self Attention 是 Attention 的一个具体做法**
- **给定一个输入 X，通过 Self Attention 得到一个 Z，这个 Z 就是对 X 的新的表征（词向量），Z 这个词向量相比较 X 拥有了句法特征和语义特征**

<img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/self-attention.jpg" alt="img" style="zoom: 67%;" />

- QKV 矩阵的由来：有输入向量 X，以及三个可学习的矩阵参数 W~Q~、W~K~、W~V~，分别用 X 右乘上述矩阵得到 Q，K，V

- **Self Attention 计算流程**

  1. QKV 获取

     <img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/qkv.jpg" alt="img" style="zoom: 67%;" />

  2. QK 点乘

      <img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/Q-K%E4%B9%98%E7%A7%AF.jpg" alt="img" style="zoom:67%;" />

    3.Softmax

    <img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/qk-scale.jpg" alt="img" style="zoom: 50%;" />

  4.对 V 加权求和

​    <img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/qk-softmax.jpg" alt="img" style="zoom:50%;" />

### 1.4 Self Attention VS RNN、LSTM

- RNN 和 LSTM 都是依次序列计算，对于远距离依赖，需要若干时间的步骤累积才能将二者联系，且距离越远有效捕获可能性越小
- 自注意力机制在计算过程中 **会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征**

### 1.5 Masked Self Attention 模型

整体架构：对比自注意力机制，就是多了一层 Mask

<img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/masked-attention.jpg" alt="img" style="zoom:67%;" />

- Mask 的作用及直观理解：不给模型看到未来的信息， **mask 就是沿着对角线把灰色的区域用 0 覆盖掉，不给模型看到未来的信息**
- 在做完 softmax 之后，横轴结果合为 1

<img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/mask-attention-map-softmax.jpg" alt="img" style="zoom:67%;" />

### 1.6 Multi-Head Self Attention 模型

<img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/multi-head-attention.png" alt="img" style="zoom:67%;" />

- head，即深度/层数，多头的个数用 h 表示，一般 h = 8
- 如何多头
  - 对于输入 X, 不是直接拿 X 去得到 Z，而是把 X 分成 8 块，得到 Z0-Z7
  - 然后将 Z0-Z7 拼起来，再做一次线性变换（改变维度），得到 Z
- **多头将 X 切分为 8 块，这样原先在一个位置上的 X，去了空间上的 8 个位置，通过对 8 个点寻找，找到更合适的位置**（用更多的矩阵去提取特征）

<img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/multi-head-%E6%8B%BC%E6%8E%A5.jpg" alt="img" style="zoom: 50%;" />

## 二、Positional Encoding

- 位置编码出现的必要性 - Attention 的缺点
  - Attention 可以并行，即在计算词向量时，词与词之间不存在顺序关系，打乱一句话，这句话里的每个词最后计算得出的词向量依然不会变，即 **无位置关系**
  - 所以通过位置编码的形式给词加一个位置
- 为什么 RNN，LSTM 不需要位置编码：因为它们本来就是依次从前往后计算的，词与词之间的顺序关系是存在的
- 看图理解：即对于输入 x1，给他加一个位置编码 t1，得到新的 x1，这样处理之后 X~i~之间就会存在位置关系

<img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/%E4%BD%8D%E7%BD%AE%E5%90%91%E9%87%8F.jpg" alt="img" style="zoom:67%;" />

- 如何加位置编码
  - pos 表示位置，i 表示维度，d~model~表示位置向量的向量维度（512）
  - 即偶数位置使用 sin 处理，奇数位置使用 cos 处理
  - **通过把单词的词向量 X，和位置编码进行叠加，这种方法就成为位置嵌入**

$$
PE_{(pos,2i)}= sin(pos/10000^{2i/d_{\mathrm{model}}})\\PE_{(pos,2i+1)}= cos(pos/10000^{2i/d_{\mathrm{model}}})
$$

<img src="C:\Users\cuihs\AppData\Roaming\Typora\typora-user-images\image-20250225163243902.png" alt="image-20250225163243902" style="zoom:67%;" />

- 原理先不看，直接记结论：**某个单词的位置信息是其他单词位置信息的线性组合** ，这种线性组合就意味着 **位置向量中蕴含** 了 **相对位置信息**。

## 三、Transformer

### 3.1 结构

<img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/tf-%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6.jpg" alt="img" style="zoom:50%;" />

- 引用 seq（编码器）2seq（解码器）模型（序列 to 序列）
- 主要就是两部分，即编码器 + 解码器
- **编码器：把输入序列变成一个词向量**
- **解码器：得到编码器输出的词向量，生成翻译的结果（单词）**

- 抽象来看，就是下面这样

<img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/tf-ed-%E6%A1%86%E6%9E%B6.jpg" alt="img" style="zoom: 67%;" />

- 在实际的 Transformer 中，编码器里面又有 N 个小编码器，一般 N = 6

<img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/tf-ed-%E5%A4%8D%E6%9D%82.jpg" alt="img" style="zoom:67%;" />

### 3.2 Encoder

- 从总结构图可以看出，每个 Encoder 有两部分组成，即多头注意力和 feedforward，在每一层之后都会加一个 **Add&Norm**，即 **残差网络+归一化**
- 整个解码器的作用：拿到初始词向量，经过 Attention 和 FeedForward 非线性变化，最终输出的词向量，相比初始词向量 **更具语义信息，包含了这个词与句子中其余词的关系**

<img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/ed-%E7%BB%86%E5%88%86.jpg" alt="img" style="zoom: 80%;" />

<img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/encoder-%E8%AF%A6%E7%BB%86%E5%9B%BE.png" alt="img" style="zoom:80%;" />

- 整个过程可以概括为

  - X~1~为 thinking 的词向量（可以是独热编码，Word2vec 等等），给 X~1~加一个位置编码得到黄色的 X~1~

  - 输入到 Self Attention 中，（X1 和 X2）拼接起来做注意力机制，得到 Z1（Z1 是 X1 和(X1, X2)做注意力机制得到的词向量，表征的仍然是 thinking），Z1 拥有了位置特征，句法特征，语义特征

  - 之后经过残差网络（避免梯度消失）以及归一化

  - 最后经过 **feedforward（两次线性变换+ReLU）**，在最后一层 Encoder 后输出最终词向量 r1

  - 注意 x，z，r 的维度都是一样的，论文中为 512 维

    <img src="C:\Users\cuihs\AppData\Roaming\Typora\typora-user-images\image-20250225193237515.png" alt="image-20250225193237515" style="zoom: 80%;" />

### 3.3Decoder

还是这个图

<img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/ed-%E7%BB%86%E5%88%86.jpg" alt="img" style="zoom:80%;" />

- 可以看到每个 decoder 包含三个子层，即自注意力、Encoder-Decoder Attention 计算和 FeedForward
- 关于为什么第一层的多头注意力机制需要 **Masked**
- 以及为什么需要计算 Encoder-Decoder Attention
- 详见 Transformer 动态流程演示

### 3.4 Transformer 的输出结果

<img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/ed-%E4%BA%A4%E4%BA%92.jpg" alt="img" style="zoom:67%;" />

- 可以看到编码器生成的词向量会传递给解码器
- 解码器得到词向量后，输出一个向量
- 这个向量会经过一层 **线性层**，最终再经过 **Softmax**
  - 线性层即全连接层，比如你的词汇表有 1000 个词，而输出向量的维度有 1500，此时就需要一个线性层将 1500 维-> 1000 维
  - 经过 Softmax，得分最高级概率最大，最终输出概率最大的词

### 3.5 Transformer 动态流程

<img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/tf-%E5%8A%A8%E6%80%81%E7%94%9F%E6%88%90.gif" alt="img" style="zoom:67%;" />

<img src="https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/tf-%E5%8A%A8%E6%80%81%E7%BB%93%E6%9E%9C-2.gif" alt="img" style="zoom:67%;" />

### 3.6 为什么解码器的 Attention 要做 Mask

- 直接原因：为了 **解决训练阶段和测试阶段的不匹配（防止过拟合）**
- 在 **每一步解码时**，模型只能利用 **已经生成的部分** 预测下一个单词，不能偷看未来单词，否则就相当于直接把答案输入模型了，模型会学得特别“投机取巧”。
- 训练阶段：解码器会有输入，这个输入就是目标语句，比如还是翻译任务，源语句是“我是一名学生”，目标语句是 "I am a student"，在训练阶段中，"I am a student" 会输入到解码器（每一次都会把句子的所有信息告诉解码器）
- 测试阶段：解码器也会有输入，但 **测试的时候不知道目标语句是什么，只会每生成一个词，就会多一个词放入目标语句**，比如在计算 am 的注意力时，能看到的只有“I”这个词。即 **计算单词依赖度时，只使用当然单词即其之前单词的词向量**，看不到后面单词
- 所以 Mask 解决了：在训练阶段时，也像测试阶段一样，生成第一个词时啥也不告诉你，生成第二个词时，告诉你第一个词....

### 3.7 为什么 Encoder 给予的是 KV 矩阵 ⭐

- 观察结构图，可以发现一个解码器中有两个 Attention 层，第一层为 Mask 多头，第二层为多头，这两层 Attention 的 QKV 也都是不同的
- **第一层 Mask 多头，其 QKV 是由目标语句的词向量分别乘可训练的矩阵 W~Q~、W~K~、W~V~得到的**
- **第二层多头，KV 矩阵是编码器给予的，Q 矩阵是上一层（Masked Self-Attention）的输出**

- 之前说过 Q 是查询变量，在这个例子中，就是已经生成的词
- K = V 是源语句
- 当我们生成词的时候，通过 **已经生成的词** 和 **源语句** 做自注意力，就是确定 **源语句中哪些词对接下来词的生成更重要**

## 四、BERT

- BERT 是从 **无标记数据集** 中训练得到的深度模型，可以显著提高 nlp 任务的准确率

### 4.1 BERT 整体模型架构

<img src="C:\Users\cuihs\AppData\Roaming\Typora\typora-user-images\image-20250226153228421.png" alt="image-20250226153228421" style="zoom:67%;" />

- BERT 使用 transformer 的 Encoder 层，**Base BERT 即 12 层 Encoder 的堆叠**

### 4.2 BERT 的输入部分

- 输入由三部分组成：Input = token embedding + segment embedding + position embedding

<img src="C:\Users\cuihs\AppData\Roaming\Typora\typora-user-images\image-20250226154031184.png" alt="image-20250226154031184" style="zoom:67%;" />

- token embedding（词嵌入）
- segment embedding
  - 用于区分输入的两个句子
  - 若是只有一个句子，则将所有的 token embedding 设为 0,
  - 若有两个句子，则第一个句子的 token embedding 设为 0，第二个句子的 token embedding 设为 1
- position embedding

